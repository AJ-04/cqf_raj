{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mrc](https://s3.eu-west-1.amazonaws.com/fl-marketing-cqf.com-001/prod/2018-09/CQF.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project - Deep Neural Networks for Solving High Dimensional PDEs in Quantitative Finance (DeepPDE)\n",
    "\n",
    "![Title Image](Figures\\BSB100D_Naisnet-Sine.png)\n",
    "\n",
    "### Raj Bharat Patel\n",
    "\n",
    "### August 2024\n",
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#1-Introduction)\n",
    "2. [Problem Statement](#2-Problem-Statement)\n",
    "3. [Theory](#3-Theory)  \n",
    "   3.1 [Neural Networks](#31-Neural-Networks)  \n",
    "   &emsp; 3.1.1 [Types of Neural Networks](#311-Types-of-Neural-Networks)  \n",
    "   &emsp; 3.1.2 [Structure of a Neural Network](#312-Structure-of-a-Neural-Network)  \n",
    "   &emsp; 3.1.3 [Forward Propagation](#313-fForward-Propagation)  \n",
    "   &emsp; 3.1.4 [Backward Propagation](#314-Backward-Propagation)  \n",
    "   3.2 [Training a Neural Network](#32-Training-a-Neural-Network)  \n",
    "   &emsp; 3.2.1 [Optimization Algorithms](#321-Optimization-Algorithms)  \n",
    "   &emsp; 3.2.2 [Regularization Techniques](#322-Regularization-Techniques)  \n",
    "   &emsp; 3.2.3 [Hyperparameter Tuning](#323-Hyperparameter-Tuning)  \n",
    "   &emsp; 3.2.4 [Model Evaluation and Metrics](#324-Model-Evaluation-and-Metrics)  \n",
    "   3.3 [Partial Differential Equations (PDEs)](#33-Partial-Differential-Equations-PDEs)  \n",
    "   &emsp; 3.3.1 [What are PDEs?](#331-What-are-PDEs)  \n",
    "   &emsp; 3.3.2 [Importance of PDEs in Quantitative Finance](#332-Importance-of-PDEs-in-Quantitative-Finance)  \n",
    "   &emsp; 3.3.3 [Solving PDEs with Neural Networks](#333-Solving-PDEs-with-Neural-Networks)  \n",
    "   &emsp; 3.3.4 [Methodologies for Solving PDEs with Neural Networks](#334-Methodologies-for-Solving-PDEs-with-Neural-Networks)  \n",
    "4. [Derivations](#4-Derivations)  \n",
    "   4.1 [Black-Scholes Equation](#41-Black-Scholes-Equation)  \n",
    "   &emsp; 4.1.1 [Log-Normal Model: Deriving the Black-Scholes Equation](#411-Log-Normal-Model-Deriving-the-Black-Scholes-Equation)  \n",
    "   &emsp; 4.1.2 [European Option Pricing and Payoff](#412-European-Option-Pricing-and-Payoff)  \n",
    "   &emsp; 4.1.3 [Delta-Hedging Portfolio](#413-Delta-Hedging-Portfolio)  \n",
    "   &emsp; 4.1.4 [Applying Itô's Lemma](#414-Applying-Itôs-Lemma)  \n",
    "   4.2 [Backward SDE and BSDE Approach](#42-Backward-SDE-and-BDSE-Approach)  \n",
    "   4.3 [FBSDE: Forward-Backward Stochastic Differential Equations and Neural Networks](#43-FBSDE-Forward-Backward-Stochastic-Differential-Equations-and-Neural-Networks)  \n",
    "   &emsp; 4.3.1 [Euler-Maruyama Scheme](#431-Euler-Maruyama-Scheme)  \n",
    "   &emsp; 4.3.2 [Loss Function](#432-Loss-Function)  \n",
    "   &emsp; 4.3.3 [Neural Network Training](#433-Neural-Network-Training)  \n",
    "5. [Architecture Modes](#5-Architecture-Modes)  \n",
    "   5.1 [Fully Connected (FC) Networks](#51-Fully-Connected-FC-Networks)  \n",
    "   5.2 [Residual Networks (ResNet)](#52-Residual-Networks-ResNet)  \n",
    "   5.3 [Nonlinear Activation Identity Scalability Network (NAIS-Net)](#53-Nonlinear-Activation-Identity-Scalability-Network-NAIS-Net)  \n",
    "6. [Testing Plan](#6-Testing-Plan)  \n",
    "7. [Results](#7-Results)  \n",
    "   7.1 [Vanilla Call Option; D = 1](#71-Vanilla-Call-Option-D-1)  \n",
    "   7.2 [Vanilla Call Option; D = 100 using Black-Scholes Barenblatt Equations](#72-Vanilla-Call-Option-D-100-using-Black-Scholes-Barenblatt-Equations)  \n",
    "   7.3 [Multi Level Monte Carlo](#73-Mulit-Level_Monte_Carlo)  \n",
    "   7.4 [Correlation of assets](#74-Correlation-of-Assets)  \n",
    "8. [Conclusion](#8-Conclusion)\n",
    "9. [References](#9-References)\n",
    "10. [Appendix](#10-Appendix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Deep learning, a subset of machine learning, has become a cornerstone of modern technology, revolutionising sectors like computer vision, natural language processing (NLP), and even self-driving cars. At the heart of this revolution are neural networks and deep neural networks (DNNs), which are designed to mimic the human brain's ability to recognise patterns and make decisions. These networks have achieved state-of-the-art performance in various tasks, such as image recognition, speech translation, and complex game strategy development.\n",
    "\n",
    "However, the adoption of deep learning techniques in the field of quantitative finance has been more conservative and gradual. While there is a growing interest in leveraging these methods, the financial industry has traditionally relied on classical mathematical models and statistical methods. Despite this, the potential for deep learning to impact quantitative finance is significant, especially given the industry's intrinsic reliance on large amounts of data and complex models.\n",
    "\n",
    "### Applications of Neural Networks in the Broader Market\n",
    "\n",
    "In the broader market, deep neural networks have demonstrated their ability to outperform traditional algorithms across a variety of domains. In computer vision, convolutional neural networks (CNNs) have set new benchmarks in image classification, object detection, and facial recognition. In NLP, recurrent neural networks (RNNs) and transformers have revolutionised tasks such as language translation, sentiment analysis, and text generation. These advances have led to the deployment of AI-driven solutions in industries ranging from healthcare, where neural networks assist in diagnosing diseases, to retail, where they optimise supply chain management and personalise customer experiences.\n",
    "\n",
    "### Applications of Neural Networks in Quantitative Finance\n",
    "\n",
    "In quantitative finance, the application of deep neural networks has been relatively limited but is gradually expanding. One of the primary uses of neural networks in this field is the development of fast pricing engines. Here, neural networks are trained to learn pricing functions by being fed prices of various financial contracts, such as options or derivatives. This approach allows for the rapid evaluation of complex instruments, which is especially useful in high-frequency trading and real-time risk management.\n",
    "\n",
    "Another emerging application is the use of DNNs in portfolio optimisation. By learning from historical market data, neural networks can identify patterns and correlations that may not be apparent through traditional methods. This can lead to more robust strategies that better navigate the complexities of financial markets.\n",
    "\n",
    "Despite these advancements, the application of deep learning to solve high-dimensional PDEs in finance remains an area with significant untapped potential. Traditional numerical methods for solving PDEs, such as finite difference or Monte Carlo simulations, often struggle with the \"curse of dimensionality\" in high-dimensional problems common in finance.\n",
    "\n",
    "Deep learning offers a promising alternative, as neural networks can be designed to approximate the solutions of these high-dimensional PDEs with greater efficiency.\n",
    "\n",
    "## 2 Problem Statement\n",
    "\n",
    "This project aims to explore how deep neural networks can be leveraged to overcome these limitations and provide efficient solutions to high-dimensional PDEs in quantitative finance and as explored by Guler, Parpas, et al. [1] and builds on earlier work by Maziar Raissi [2].\n",
    "\n",
    "The task is to develop a Deep Neural Network (DNN) to solve *d*-dimensional PDEs:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + \\frac{1}{2} \\text{Tr} \\left( \\sigma(t, x) \\sigma(t, x)^\\top \\nabla^2 u(t, x) \\right) + \\nabla u(t, x)^\\top \\mu(t, x) + f(t, x, u(t, x), \\sigma(t, x)^\\top \\nabla u(t, x)) = 0 \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ u (T, x_1, \\dots, x_d) = g(x_1, \\dots, x_d) \\tag{2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $u(t, x)$ with $t \\in \\mathbb{R}_+$, and $x \\in X \\subset \\mathbb{R}^d$ is the unknown function (e.g., option price).\n",
    "- $\\nabla u(t, x)$ is the gradient of the solution with respect to $x$.\n",
    "- $\\nabla^2 u(t, x)$ is the Hessian matrix (again with respect to $x$).\n",
    "- $\\sigma(t, x)$ is a known $d \\times d$ matrix-valued function.\n",
    "- $\\text{Tr}$ denotes the trace of a matrix and $\\top$ denotes a transpose.\n",
    "- $f$ is a known nonlinear function.\n",
    "- $g$ is a known boundary condition.\n",
    "\n",
    "Solving the equation above is not realistic in the time-frame of a CQF project, so instead we consider the simpler case:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} + \\sum_{i=1}^{d} \\left( \\frac{1}{2} \\sigma_i^2 \\frac{\\partial^2 u}{\\partial x_i^2} x_i^2 + r \\frac{\\partial u}{\\partial x_i} x_i \\right) - r u(t, x) = 0 \\tag{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "u(T, x_1, \\dots, x_d) = g(x_1, \\dots, x_d) \\tag{4}\n",
    "$$\n",
    "\n",
    "- If $d = 1$ and $g(x) = \\max(x - K, 0)$ then this is just the Black-Scholes PDE for a vanilla call option with strike $K$ and maturity $T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Theory\n",
    "\n",
    "### 3.1 Neural Networks\n",
    "\n",
    "Neural networks are computational models inspired by the human brain's structure and function. They consist of interconnected layers of nodes (or neurons) that process and transform input data to generate an output. The power of neural networks lies in their ability to learn complex patterns and relationships in data through training.\n",
    "\n",
    "#### 3.1.1 Types of Neural Networks\n",
    "\n",
    "Different types of neural networks are designed to handle different types of data:\n",
    "\n",
    "- **Feedforward Neural Networks (FNNs):** The simplest type, where the data moves in one direction—from the input layer to the output layer—without any cycles or loops.\n",
    "  \n",
    "- **Convolutional Neural Networks (CNNs):** Specialised for processing grid-like data, such as images, by applying convolutional filters to capture spatial hierarchies of features.\n",
    "\n",
    "- **Recurrent Neural Networks (RNNs):** Designed for sequential data, like time series or text, by maintaining a hidden state that captures information from previous inputs.\n",
    "\n",
    "- **Transformers:** A more recent architecture primarily used in NLP, characterised by self-attention mechanisms that allow the network to weigh the importance of different inputs dynamically.\n",
    "\n",
    "#### 3.1.2 Structure of a Neural Network\n",
    "\n",
    "A typical neural network is composed of three types of layers:\n",
    "\n",
    "1. **Input Layer:** The input layer takes the raw data that the network will process. Each neuron in this layer represents a feature or variable from the input dataset.\n",
    "\n",
    "2. **Hidden Layers:** Hidden layers are the layers between the input and output layers. Each hidden layer consists of neurons that apply transformations to the input data, progressively extracting higher-level features. These transformations are governed by weights and biases, which are adjusted during training. The number of hidden layers and neurons in each layer define the depth and capacity of the network.\n",
    "\n",
    "3. **Output Layer:** The output layer produces the final result of the network's computations. The number of neurons in this layer corresponds to the desired output, such as a class label in classification tasks or a predicted value in regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Forward Propagation\n",
    "\n",
    "![Title Image](Figures\\forwardpropagation.png)\n",
    "*Figure 1: Simple Perceptron - CQF lecture on Introduction to Deep Learning and Nueral Networks*\n",
    "\n",
    "\n",
    "During forward propagation, the input data is passed through each layer of the network, and each neuron performs the following computation:\n",
    "\n",
    "$$\n",
    "z_j^{(l)} = \\sum_{i=1}^{n^{(l-1)}} w_{ji}^{(l)} a_i^{(l-1)} + b_j^{(l)} \\tag{5}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $z_j^{(l)}$ is the weighted sum of inputs for the $j$-th neuron in layer $l$.\n",
    "- $w_{ji}^{(l)}$ represents the weight from the $i$-th neuron in the previous layer $(l-1)$ to the $j$-th neuron in the current layer $l$.\n",
    "- $a_i^{(l-1)}$ is the activation of the $i$-th neuron in the previous layer.\n",
    "- $b_j^{(l)}$ is the bias term for the $j$-th neuron in layer $l$.\n",
    "- $n^{(l-1)}$ is the number of neurons in the previous layer.\n",
    "\n",
    "The neuron then applies an activation function $\\phi$ to this weighted sum to produce its output:\n",
    "\n",
    "$$\n",
    "a_j^{(l)} = \\phi(z_j^{(l)}) \\tag{6}\n",
    "$$\n",
    "\n",
    "Common activation functions include:\n",
    "\n",
    "- **Sine:**\n",
    "  $$\n",
    "  \\phi(z) = \\sin(z) \\tag{7}\n",
    "  $$\n",
    "  The sine function is periodic and oscillates between -1 and 1. While not as commonly used as other activation functions, sine functions can be useful in neural networks designed to model periodic or oscillatory data. However, its non-monotonic nature can introduce challenges in training, such as the potential for the network to get stuck in local minima during optimisation.\n",
    "\n",
    "- **Sigmoid:**\n",
    "  $$\n",
    "  \\phi(z) = \\frac{1}{1 + e^{-z}} \\tag{8}\n",
    "  $$\n",
    "  The sigmoid function maps input values to an output range between 0 and 1, making it useful in scenarios where outputs need to be interpreted as probabilities, such as binary classification tasks. Its \"S\"-shaped curve allows for smooth transitions between output values. However, sigmoid functions can suffer from vanishing gradient issues, especially in deep networks, which can slow down the training process.\n",
    "\n",
    "- **ReLU (Rectified Linear Unit):**\n",
    "  $$\n",
    "  \\phi(z) = \\max(0, z) \\tag{9}\n",
    "  $$\n",
    "  The ReLU function outputs the input directly if it is positive, otherwise it outputs zero. This simple, piecewise linear function has become the default activation function for many neural network architectures due to its computational efficiency and ability to mitigate the vanishing gradient problem. However, ReLU can lead to \"dead\" neurons, where neurons get stuck outputting zero for all inputs and cease learning.\n",
    "\n",
    "- **Tanh:**\n",
    "  $$\n",
    "  \\phi(z) = \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}} \\tag{10}\n",
    "  $$\n",
    "  The tanh function, like the sigmoid function, is \"S\"-shaped but maps input values to an output range between -1 and 1. This zero-centered output can make training more efficient in some cases compared to the sigmoid function. However, similar to the sigmoid, tanh functions can also suffer from vanishing gradient issues, which can hinder the training of deep networks.\n",
    "\n",
    "The final output of the network is produced after forward propagation through all the layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Backward Propagation\n",
    "\n",
    "Backward Propagation, is a fundamental algorithm used in training neural networks. It is responsible for updating the weights of the network to minimise the loss function, thereby improving the accuracy of the network's predictions over time. Backpropagation works by propagating the error backward through the network, from the output layer to the input layer, and adjusting the weights based on the computed gradients.\n",
    "\n",
    "The process begins by calculating the loss function $\\mathcal{L}(\\hat{y}, y)$, which measures the difference between the predicted output $\\hat{y}$ and the actual target $y$. The choice of loss function depends on the task:\n",
    "  - For regression tasks, a common loss function is the Mean Squared Error (MSE):\n",
    "     $$\n",
    "     \\mathcal{L}(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 \\tag{11}\n",
    "     $$\n",
    "  - For classification tasks, Cross-Entropy Loss is commonly used:\n",
    "     $$\n",
    "     \\mathcal{L}(y, \\hat{y}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\tag{12}\n",
    "     $$\n",
    "\n",
    "The next step is to compute the gradient of the loss function with respect to each weight in the network using the chain rule. For each weight $w_{ji}^{(l)}$ in the network, the gradient is calculated as:\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial w_{ji}^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial a_j^{(l)}} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\cdot \\frac{\\partial z_j^{(l)}}{\\partial w_{ji}^{(l)}} \\tag{13}\n",
    "   $$\n",
    "   Where:\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial w_{ji}^{(l)}}$ is the gradient of the loss with respect to the weight $w_{ji}^{(l)}$.\n",
    "   - $\\frac{\\partial \\mathcal{L}}{\\partial a_j^{(l)}}$ is the gradient of the loss with respect to the activation $a_j^{(l)}$ at the $j$-th neuron in layer $l$.\n",
    "   - $\\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}$ is the derivative of the activation function with respect to the weighted sum $z_j^{(l)}$.\n",
    "   - $\\frac{\\partial z_j^{(l)}}{\\partial w_{ji}^{(l)}}$ is the derivative of the weighted sum with respect to the weight $w_{ji}^{(l)}$.\n",
    "\n",
    "The gradients are propagated backward through the network, layer by layer, from the output layer to the input layer. Once the gradients are calculated, the weights are updated to minimise the loss function. The weights are adjusted in the opposite direction of the gradient (hence \"gradient descent\") by an amount proportional to the learning rate $\\eta$:\n",
    "   $$\n",
    "   w_{ji}^{(l)} := w_{ji}^{(l)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w_{ji}^{(l)}} \\tag{14}\n",
    "   $$\n",
    "   Where:\n",
    "   - $w_{ji}^{(l)}$ is the weight from neuron $i$ in layer $(l-1)$ to neuron $j$ in layer $l$.\n",
    "   - $\\eta$ is the learning rate, a hyperparameter that controls the step size of the weight update.\n",
    "\n",
    "This update rule is applied iteratively over the entire dataset (or a batch of data in the case of mini-batch gradient descent) until the network converges, i.e., when the loss function reaches a minimum or a predefined stopping criterion is met.\n",
    "\n",
    "The backpropagation process is repeated across multiple iterations (epochs) of the training data, with the goal of minimising the loss function and improving the accuracy of the network's predictions. Backpropagation ensures that the network learns the correct mapping from inputs to outputs.\n",
    "\n",
    "While backpropagation is a powerful algorithm, it can face several challenges:\n",
    "\n",
    "- **Vanishing and Exploding Gradients:** In very deep networks, gradients can become extremely small (vanishing) or extremely large (exploding) as they are propagated back through the layers. This can hinder the learning process, making it difficult for the network to converge. Techniques like ReLU activation, batch normalisation, and gradient clipping are often used to mitigate these issues.\n",
    "\n",
    "- **Local Minima and Saddle Points:** The optimisation landscape of neural networks is highly non-convex, meaning there are many local minima and saddle points. The network might get stuck in these regions, preventing it from reaching the global minimum. Advanced optimisation algorithms like Adam, RMSprop, and momentum-based methods help the network navigate these challenges more effectively.\n",
    "\n",
    "- **Learning Rate Sensitivity:** The choice of learning rate is crucial for the success of backpropagation. A learning rate that is too high can cause the training process to overshoot the minimum, while a learning rate that is too low can lead to slow convergence. Adaptive learning rate methods and learning rate schedules are often employed to fine-tune this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Training a Neural Network\n",
    "\n",
    "The primary goal of training a neural network is to minimise the difference between the predicted output $\\hat{y}$ and the actual target values $y$. This is quantified using a **loss function** $\\mathcal{L}$, which measures the error of the network's predictions.\n",
    "\n",
    "Common loss functions include:\n",
    "\n",
    "- **Mean Squared Error (MSE):**\n",
    "  $$\n",
    "  \\mathcal{L}(y, \\hat{y}) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 \\tag{15}\n",
    "  $$\n",
    "  Used in regression tasks, where $m$ is the number of samples.\n",
    "\n",
    "- **Cross-Entropy Loss:**\n",
    "  $$\n",
    "  \\mathcal{L}(y, \\hat{y}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] \\tag{16}\n",
    "  $$\n",
    "  Used in binary classification tasks.\n",
    "\n",
    "To minimise the loss, neural networks use **backpropagation** to compute the gradient of the loss function with respect to each weight. This is done using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{ji}^{(l)}} = \\frac{\\partial \\mathcal{L}}{\\partial a_j^{(l)}} \\cdot \\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}} \\cdot \\frac{\\partial z_j^{(l)}}{\\partial w_{ji}^{(l)}} \\tag{17}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial w_{ji}^{(l)}}$ is the gradient of the loss with respect to the weight $w_{ji}^{(l)}$.\n",
    "- $\\frac{\\partial \\mathcal{L}}{\\partial a_j^{(l)}}$ is the gradient of the loss with respect to the activation of the neuron.\n",
    "- $\\frac{\\partial a_j^{(l)}}{\\partial z_j^{(l)}}$ is the derivative of the activation function.\n",
    "- $\\frac{\\partial z_j^{(l)}}{\\partial w_{ji}^{(l)}}$ is the derivative of the weighted sum with respect to the weight.\n",
    "\n",
    "The weights are then updated using an optimisation algorithm, typically **gradient descent**:\n",
    "\n",
    "$$\n",
    "w_{ji}^{(l)} := w_{ji}^{(l)} - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w_{ji}^{(l)}} \\tag{18}\n",
    "$$\n",
    "\n",
    "Where $\\eta$ is the learning rate, a hyperparameter that controls the step size of the weight update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Optimisation Algorithms\n",
    "\n",
    "The choice of optimisation algorithm is crucial for the efficient training of neural networks. Different variants of gradient descent and advanced optimisation methods can significantly impact the convergence speed and final model performance.\n",
    "\n",
    "- **Stochastic Gradient Descent (SGD):** An optimisation method where gradients are computed and weights are updated for each training sample individually. This can make the training process faster but also noisier, potentially leading to more fluctuation in the loss curve.\n",
    "  \n",
    "- **Mini-Batch Gradient Descent:** A compromise between SGD and full-batch gradient descent, where gradients are computed on small batches of data. This method balances computational efficiency with the stability of the training process.\n",
    "\n",
    "- **Adam:** An adaptive learning rate optimisation algorithm that combines the benefits of two other extensions of SGD, namely AdaGrad and RMSprop. It computes individual adaptive learning rates for different parameters and is widely used for training deep neural networks.\n",
    "\n",
    "#### 3.2.2 Regularisation Techniques\n",
    "\n",
    "To prevent overfitting, various regularisation techniques are employed during neural network training:\n",
    "\n",
    "- **L1/L2 Regularisation:** Adds a penalty term to the loss function proportional to the magnitude of the model's weights (L1 for absolute values, L2 for squared values), encouraging the network to keep weights small and reducing overfitting.\n",
    "\n",
    "- **Dropout:** A technique where randomly selected neurons are ignored during training. This prevents the network from becoming too reliant on specific neurons, effectively making it more robust and reducing overfitting.\n",
    "\n",
    "- **Early Stopping:** A method where training is stopped as soon as the model's performance on a validation set begins to degrade, preventing overfitting to the training data.\n",
    "\n",
    "#### 3.2.3 Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is critical for optimising neural network performance. Methods include:\n",
    "\n",
    "- **Grid Search and Random Search:** Traditional methods for exploring a predefined set of hyperparameters to identify the best combination.\n",
    "  \n",
    "- **Bayesian Optimisation:** A more sophisticated approach that models the performance of the hyperparameters and uses this model to explore the space more efficiently.\n",
    "  \n",
    "- **Learning Rate Schedulers:** Techniques to adjust the learning rate during training, such as step decay, exponential decay, and cyclic learning rates, which can help achieve better convergence.\n",
    "\n",
    "#### 3.2.4 Model Evaluation and Metrics\n",
    "\n",
    "Evaluating a neural network's performance is crucial to understanding how well it generalises to new data. Common metrics include:\n",
    "\n",
    "- **Accuracy:** The proportion of correct predictions over the total number of predictions, commonly used in classification tasks.\n",
    "  \n",
    "- **Precision, Recall, and F1 Score:** Metrics used in classification tasks to measure the model's performance in imbalanced datasets.\n",
    "  \n",
    "- **Root Mean Squared Error (RMSE):** A standard metric for regression tasks, representing the square root of the average squared differences between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Partial Differential Equations (PDEs)\n",
    "\n",
    "#### 3.3.1 What are PDEs?\n",
    "\n",
    "Partial Differential Equations (PDEs) are a type of differential equation that involve multiple independent variables, their partial derivatives, and an unknown function. PDEs are fundamental in the modeling of various physical, financial and engineering systems where changes in a system depend on multiple factors. For example, PDEs are widely used in fields such as physics (e.g., heat conduction, wave propagation), engineering (e.g., fluid dynamics), and finance (e.g., option pricing).\n",
    "\n",
    "A general form of a second-order PDE in two variables is:\n",
    "\n",
    "$$\n",
    "F\\left(x, y, u, \\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y}, \\frac{\\partial^2 u}{\\partial x^2}, \\frac{\\partial^2 u}{\\partial y^2}\\right) = 0 \\tag{19}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $u$ is the unknown function of $x$ and $y$.\n",
    "- $\\frac{\\partial u}{\\partial x}$ and $\\frac{\\partial u}{\\partial y}$ are the first-order partial derivatives.\n",
    "- $\\frac{\\partial^2 u}{\\partial x^2}$ and $\\frac{\\partial^2 u}{\\partial y^2}$ are the second-order partial derivatives.\n",
    "\n",
    "#### 3.3.2 Importance of PDEs in Quantitative Finance\n",
    "\n",
    "In quantitative finance, PDEs are crucial for modeling the evolution of financial variables over time. One of the most famous PDEs in finance is the **Black-Scholes equation**, which models the price evolution of European options. The Black-Scholes equation is a second-order parabolic PDE given by:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial V}{\\partial t} + \\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 V}{\\partial S^2} + r S \\frac{\\partial V}{\\partial S} - r V = 0 \\tag{20}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $V(S, t)$ is the price of the option as a function of the underlying asset price $S$ and time $t$.\n",
    "- $\\sigma$ is the volatility of the asset.\n",
    "- $r$ is the risk-free interest rate.\n",
    "\n",
    "PDEs like the Black-Scholes equation provide the foundation for pricing derivatives, managing risk, and optimising portfolios in financial markets.\n",
    "\n",
    "#### 3.3.3 Solving PDEs with Neural Networks\n",
    "\n",
    "Traditional methods for solving PDEs, such as finite difference methods, finite element methods, and Monte Carlo simulations, often struggle with the **curse of dimensionality**. As the number of dimensions (variables) in a PDE increases, the computational complexity grows exponentially, making these methods inefficient or impractical for high-dimensional problems.\n",
    "\n",
    "Neural Networks (NNs), particularly Deep Neural Networks (DNNs), offer a promising alternative for solving high-dimensional PDEs. The key advantages of using NNs include:\n",
    "\n",
    "- **Approximation Power:** NNs can approximate complex functions with high accuracy, making them well-suited for capturing the solutions of PDEs.\n",
    "- **Scalability:** NNs can handle high-dimensional problems more efficiently than traditional methods, as they do not rely on discretising the entire problem space.\n",
    "- **Generalisation:** Once trained, NNs can provide solutions for a wide range of inputs, offering greater flexibility in applications where parameters may vary.\n",
    "\n",
    "#### 3.3.4 Methodologies for Solving PDEs with Neural Networks\n",
    "\n",
    "There are several methodologies for solving PDEs using Neural Networks. The primary approaches include:\n",
    "\n",
    "1. **Physics-Informed Neural Networks (PINNs):** PINNs incorporate the governing PDE directly into the loss function of the neural network. This ensures that the network's predictions satisfy the PDE, making it a powerful method for solving both forward and inverse problems.\n",
    "\n",
    "2. **Deep Galerkin Method (DGM):** DGM is an extension of the Galerkin method to deep learning, where the solution is approximated by a neural network, and the loss function is constructed to minimise the residual of the PDE.\n",
    "\n",
    "3. **Backward Stochastic Differential Equations (BSDE) Approach:** BSDEs are a type of stochastic differential equation used in finance to represent the price of contingent claims. Neural networks can be employed to solve BSDEs by approximating the solution of the associated PDE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Derivations\n",
    "\n",
    "### 4.1 Black-Scholes Equation\n",
    "\n",
    "### 4.1.1 Log-Normal Model: Deriving the Black-Scholes Equation\n",
    "\n",
    "We start by considering a financial model where the price of an asset follows a geometric Brownian motion, also known as a log-normal model. The stochastic differential equation (SDE) that describes this model is given by:\n",
    "\n",
    "$$\n",
    "dX_t = rX_t dt + \\sigma X_t dW_t, \\quad X_0 = x \\tag{21}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $r$ is the risk-free interest rate.\n",
    "- $X_t$ is the current price of the asset at time $t$.\n",
    "- $\\sigma$ is the volatility of the asset.\n",
    "- $dW_t$ is the increment of a Wiener process (Brownian motion).\n",
    "\n",
    "This equation models the evolution of the asset price under the assumption of continuous trading and no arbitrage.\n",
    "\n",
    "### 4.1.2 European Option Pricing and Payoff\n",
    "\n",
    "Consider a European-style option with a payoff at maturity $T$ given by:\n",
    "\n",
    "$$\n",
    "Y_T(X) = g(X) \\tag{22}\n",
    "$$\n",
    "\n",
    "Where $g(X)$ is a known payoff function (e.g., $g(X) = \\max(X - K, 0)$ for a European call option). Our goal is to compute the value of the option at any time $t < T$, denoted as $u_t(X) = Y_t$.\n",
    "\n",
    "### 4.1.3 Delta-Hedging Portfolio\n",
    "\n",
    "To derive the Black-Scholes PDE, we consider a delta-hedging strategy where we form a portfolio $\\Pi_t$ consisting of the option and the underlying asset. The value of the portfolio at time $t$ is:\n",
    "\n",
    "$$\n",
    "\\Pi_t = Y_t - \\Delta X_t \\tag{23}\n",
    "$$\n",
    "\n",
    "Where $\\Delta$ is the amount of the underlying asset held in the portfolio, chosen to eliminate the risk from the random movement of $X_t$.\n",
    "\n",
    "### 4.1.4 Applying Itô's Lemma\n",
    "\n",
    "To find the dynamics of the portfolio, we apply Itô's Lemma to $Y_t$, which describes how a function of a stochastic process evolves. The differential of the portfolio value is:\n",
    "\n",
    "$$\n",
    "d\\Pi_t = dY_t - \\Delta dX_t \\tag{24}\n",
    "$$\n",
    "\n",
    "Expanding $dY_t$ using Itô's Lemma:\n",
    "\n",
    "$$\n",
    "dY_t = \\left(\\frac{\\partial Y}{\\partial t} + \\frac{1}{2} \\sigma^2 X^2 \\frac{\\partial^2 Y}{\\partial X^2}\\right) dt + \\frac{\\partial Y}{\\partial X} dX \\tag{25}\n",
    "$$\n",
    "\n",
    "Substituting this into the expression for $d\\Pi_t$:\n",
    "\n",
    "$$\n",
    "d\\Pi_t = \\left(\\frac{\\partial Y}{\\partial t} + \\frac{1}{2} \\sigma^2 X^2 \\frac{\\partial^2 Y}{\\partial X^2}\\right) dt + \\frac{\\partial Y}{\\partial X} dX - \\Delta dX \\tag{26}\n",
    "$$\n",
    "\n",
    "To eliminate the stochastic term $dX_t$, we choose $\\Delta = \\frac{\\partial Y}{\\partial X}$, resulting in:\n",
    "\n",
    "$$\n",
    "d\\Pi_t = \\left(\\frac{\\partial Y}{\\partial t} + \\frac{1}{2} \\sigma^2 X^2 \\frac{\\partial^2 Y}{\\partial X^2}\\right) dt \\tag{27}\n",
    "$$\n",
    "\n",
    "The portfolio $\\Pi_t$ must earn the risk-free rate $r$, leading to the condition:\n",
    "\n",
    "$$\n",
    "d\\Pi_t = r\\Pi_t dt \\tag{28}\n",
    "$$\n",
    "\n",
    "Substituting $\\Pi_t = Y_t - \\frac{\\partial Y}{\\partial X} X_t$:\n",
    "\n",
    "$$\n",
    "\\left(\\frac{\\partial Y}{\\partial t} + \\frac{1}{2} \\sigma^2 X^2 \\frac{\\partial^2 Y}{\\partial X^2}\\right) dt = r \\left(Y_t - \\frac{\\partial Y}{\\partial X} X_t\\right) dt \\tag{29}\n",
    "$$\n",
    "\n",
    "This simplifies to the Black-Scholes PDE:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Y}{\\partial t} + \\frac{1}{2} \\sigma^2 X^2 \\frac{\\partial^2 Y}{\\partial X^2} + rX \\frac{\\partial Y}{\\partial X} - rY = 0 \\tag{30}\n",
    "$$\n",
    "\n",
    "## 4.2 Backward SDE and BSDE Approach\n",
    "\n",
    "To derive the Backward Stochastic Differential Equation (BSDE), we apply Itô’s lemma to the process $Y_t$:\n",
    "\n",
    "$$\n",
    "dY = \\left(\\frac{\\partial Y}{\\partial t} + \\frac{1}{2} \\sigma^2 X^2 \\frac{\\partial^2 Y}{\\partial X^2}\\right) dt + \\frac{\\partial Y}{\\partial X} dX \\tag{31}\n",
    "$$\n",
    "\n",
    "Substituting the SDE for $dX_t$:\n",
    "\n",
    "$$\n",
    "dY = \\left(\\frac{\\partial Y}{\\partial t} + \\frac{1}{2} \\sigma^2 X^2 \\frac{\\partial^2 Y}{\\partial X^2}\\right) dt + \\frac{\\partial Y}{\\partial X} \\left(rX dt + \\sigma X dW_t\\right) \\tag{32}\n",
    "$$\n",
    "\n",
    "Simplifying, we obtain:\n",
    "\n",
    "$$\n",
    "dY = \\left(rY - rX \\frac{\\partial Y}{\\partial X}\\right) dt + \\frac{\\partial Y}{\\partial X} \\sigma X dW_t \\tag{33}\n",
    "$$\n",
    "\n",
    "This equation is the BSDE associated with the Black-Scholes PDE. The key idea is that BSDEs can be solved using neural networks by parameterizing $Y_t$ and $\\frac{\\partial Y}{\\partial X}$ using neural networks and training them to satisfy the SDE/BSDE and PDE using backpropagation.\n",
    "\n",
    "## 4.3 FBSDE: Forward-Backward Stochastic Differential Equations and Neural networks\n",
    "\n",
    "Forward-Backward Stochastic Differential Equations (FBSDEs) are a powerful framework for solving high-dimensional PDEs, particularly those arising in finance. The forward SDE typically models the evolution of a state variable (e.g., the price of an asset), while the backward SDE models the evolution of an associated value function (e.g., the price of a derivative). By applying the correct time discretisation, we can use neural networks to approximate the solution to an FBSDE, with the aim to minimise the loss function between this approximation and an \"exact\" value.\n",
    "\n",
    "Let us consider the following FBSDE can be generally expressed as:\n",
    "\n",
    "$$\n",
    "dX_t = \\mu(t, X_t, Y_t, Z_t) dt + \\sigma(t, X_t, Y_t) dW_t, \\quad t \\in [0, T], \\tag{34}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_0 = \\xi, \\tag{35}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dY_t = \\phi(t, X_t, Y_t, Z_t) dt + Z_t' \\sigma(t, X_t, Y_t) dW_t, \\quad t \\in [0, T], \\tag{36}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_T = g(X_T), \\tag{37}\n",
    "$$\n",
    "\n",
    "where $X_t$, $Y_t$, and $Z_t$ are different stochastic processes, $W_t$ is a Wiener process, $\\mu$ and $\\phi$ represent drift functions, and $\\sigma$ is the diffusion function for the processes. The solution to this set of equations can be linked to the solution of a partial differential equation of the form\n",
    "\n",
    "$$\n",
    "u_t = f(t, x, u, Du, D^2u) \\tag{38}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\phi(t, x, u, Du) - \\mu(t, x, u, Du)' Du - \\frac{1}{2} \\text{Tr}[\\sigma(t, x, u)\\sigma(t, x, u)'D^2 u] \\tag{39}\n",
    "$$\n",
    "\n",
    "where $Du$ represents the gradient, $D^2u$ represents the Hessian of $u$ respectively with $Y_t = u(t, X_t)$ and $Z_t = Du(t, X_t)$ and the terminal condition of $u$ is known that is, $u(T, X_T) = g(X_T)$. \n",
    "\n",
    "We can approximate the function $u(t, X_t)$ using a neural network using the PyTorch library. The terms $Du$ and $D^2u$ are calculated using the automatic differentiation capability of this library.\n",
    "\n",
    "### 4.3.1 Euler-Maruyama Scheme\n",
    "\n",
    "We now discretise the equations in FBSDE equations using the Euler-Maruyama scheme to get the following set of equations:\n",
    "\n",
    "$$\n",
    "\\Delta W_n \\sim \\mathcal{N}(0, \\Delta t_n), \\tag{40}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_{n+1} \\approx X_n + \\mu(t_n, X_n, Y_n, Z_n) \\Delta t_n + \\sigma(t_n, X_n, Y_n) \\Delta W_n, \\tag{41}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_{n+1} \\approx Y_n + \\phi(t_n, X_n, Y_n, Z_n) \\Delta t_n + (Z_n)'\\sigma(t_n, X_n, Y_n) \\Delta W_n, \\tag{42}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\Delta t_n = t_{n+1} - t_n = \\frac{T}{N}, \\tag{43}\n",
    "$$\n",
    "\n",
    "for $n = 0, 1, 2, \\dots, N - 1$, where $N$ is the number of time-steps.\n",
    "\n",
    "### 4.3.2 Loss Function\n",
    "\n",
    "The neural network predicts $Y_n$ for each input $(t_n, X_n)$. The loss function compares this prediction of the neural network with the actual value calculated by the Euler-Maryuama scheme discretization step using a sum squared error loss for $N$ time-steps and $M$ realizations of the Wiener process $W$ also known as batch size is given by:\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^{M} \\sum_{n=0}^{N-1} |Y_{n+1}^m - Y_n^m - \\Phi_n^m \\Delta t_n - (Z_n^m)' \\Sigma_n^m \\Delta W_n^m|^2 \\tag{44}\n",
    "$$\n",
    "\n",
    "$$\n",
    "+ \\sum_{m=1}^{M} |Y_n^m - g(X_N^m)|^2 + \\sum_{m=1}^{M} |Z_N^m - g'(X_N^m)|^2 \\tag{3.4} \\tag{45}\n",
    "$$\n",
    "\n",
    "where $\\Phi_n^m = \\phi(t_n, X_n^m, Y_n^m, Z_n^m)$ and $\\Sigma_n^m = \\sigma(t_n, X_n^m, Y_n^m)$.\n",
    "\n",
    "From the equations in the Euler-Maryuama scheme discretization step we also have\n",
    "\n",
    "$$\n",
    "X_{n+1}^m = X_n^m + \\mu(t_n^m, X_n^m, Y_n^m, Z_n^m) \\Delta t_n + \\sigma(t_n^m, X_n^m, Y_n^m) \\Delta W_n^m, \\tag{46}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_n^m = u(t_n^m, X_n^m), \\tag{47}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z_n^m = Du(t_n^m, X_n^m), \\tag{48}\n",
    "$$\n",
    "\n",
    "with $X_0^m = \\xi$ for all $m$.\n",
    "\n",
    "### 4.3.3 Neural Network Training\n",
    "\n",
    "To train the neural network, the process begins by initializing the model with its attributes: $\\xi$, $T$, $M$, $N$, $D$, and layers. The training proceeds over a set number of iterations. For each iteration, $M$ realizations of the Wiener Process $W$ and time steps $\\Delta t$ are generated, and the loss is initialized to zero.\n",
    "\n",
    "Next, the model calculates the values for $M$ realizations of $Y_0$ and $Z_0$ by passing the initial conditions $(t_0, \\xi)$ through the model using automatic differentiation. For each time step $n$ from 1 to $N-1$, the values of $X_{n+1}$ and the true value $\\hat{Y}_{n+1}$ are obtained using $\\Delta t_n$, $X_n$, $W_n$, $Y_n$, and $Z_n$ through the equations 3.3. The model’s predicted values $Y_{n+1}$ and $Z_{n+1}$ are then calculated by passing $(t_n, X_n)$ through the model with automatic differentiation.\n",
    "\n",
    "The loss is updated by adding the sum squared error between the true and predicted values across all realizations. Additionally, the loss for the terminal time $T$ is computed by adding the squared differences between the model predictions and the terminal condition $g(X_N^m)$, as well as the difference in the derivatives $g'(X_N^m)$.\n",
    "\n",
    "The gradients across the neural network are computed using backpropagation, and the model then minimizes the loss using an appropriate optimizer. Finally, the weights of the network are updated to prepare for the next iteration.\n",
    "\n",
    "This iterative process continues until the model has undergone the specified number of iterations, progressively refining its predictions and minimizing the loss function to achieve the best possible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Architecture Modes\n",
    "\n",
    "In the design of neural networks, the choice of architecture plays a critical role in determining the network's performance and efficiency. Various architectural modes have been developed to address specific challenges in training deep networks. This section explores three important architectures: Fully Connected (FC) networks, Residual Networks (ResNet), and Nonlinear Activation Identity Scalability Network (NAIS-Net).\n",
    "\n",
    "### 5.1 Fully Connected (FC) Networks\n",
    "\n",
    "Fully Connected networks, also known as dense networks, are the most straightforward type of neural network architecture.\n",
    "\n",
    "- **Layer Connectivity:** Each neuron receives input from every neuron in the previous layer and passes its output to every neuron in the next layer.\n",
    "- **Parameter Count:** The number of parameters (weights and biases) in a Fully Connected network can be very large, especially for networks with many layers and neurons, leading to significant computational demands.\n",
    "- **Versatility:** Fully Connected networks are highly versatile and can be used for a wide range of tasks, including classification, regression, and function approximation. However, they can be prone to overfitting, particularly when trained on small datasets.\n",
    "\n",
    "In our setup, the neural network contains 4 hidden layers, each with L = 256 neurons. The input layer has $D + 1$ neurons where $D$ is the number of dimensions of the $X_t$. The output layer contains the prediction $Y_t$.\n",
    "\n",
    "![FCNN](Figures\\Fully-connected-deep-neural-network.png)\n",
    "*Figure 2: Fully Connected Neural Network (FCNN) with 3 hidden layers*\n",
    "\n",
    "### 5.2 Residual Networks (ResNet)\n",
    "\n",
    "Residual Networks (ResNet) were proposed in 2015 by He et al. [4] to address the degradation problem in deep neural networks. As networks become deeper, they can suffer from vanishing or exploding gradients, making training difficult. ResNet introduced the concept of:\n",
    "\n",
    "- **Identity Mappings:** ResNet uses identity mappings (shortcut connections) that bypass one or more layers, adding the input of a layer directly to its output. If the input to a layer is denoted as $x$, and the function learned by the layer is $F(x)$, the output of the layer in ResNet would be $y = F(x) + x$.\n",
    "- **Gradient Flow:** These shortcut connections facilitate the flow of gradients during backpropagation, helping to mitigate the vanishing gradient problem and enabling the training of very deep networks (e.g., networks with over 100 layers).\n",
    "- **Performance:** ResNet has been highly successful in various computer vision tasks, such as image classification and object detection, and has become a foundational architecture in the development of even more advanced networks.\n",
    "\n",
    "### 5.3 Nonlinear Activation Identity Scalability Network (NAIS-Net)\n",
    "\n",
    "NAIS-Net, or Nonlinear Activation Identity Scalability Network, was proposed to overcome the problem of forward stability in Residual Networks by Ciccone et al. [5].\n",
    "\n",
    "![FCNN](Figures\\naisnet_blockdiag.png)\n",
    "*Figure 3: NAIS-Net architecture*\n",
    "\n",
    "Each block represents a time-invariant iterative process as the first layer in the i-th block, $x_i$, is unrolled into a pattern-dependent number, $K_i$, of processing stages, using weight matrices $A_i$ and $B_i$. The skip connections from the input, $u_i$, to all layers in block $i$ make the process nonautonomous. Blocks can be chained together (each block modeling a different latent space) by passing final latent representation, $x_i(K_i)$, of block $i as the input to block $i + 1$.\n",
    "\n",
    "Some benefits of this approach include:\n",
    "\n",
    "- **Scalable Identity Mappings:** Unlike the fixed identity mappings in ResNet, NAIS-Net introduces a scaling factor to the identity path. This allows the network to balance the contribution of the identity connection and the learned transformation, leading to more stable forward propagation.\n",
    "- **Forward Stability:** By dynamically adjusting the scaling of identity mappings, NAIS-Net helps maintain the stability of the network’s outputs as they propagate through the layers, reducing the risk of exploding outputs in deep networks.\n",
    "- **Adaptability:** The ability to scale the identity path makes NAIS-Net adaptable to different tasks and network depths, offering more flexibility than standard ResNet architectures.\n",
    "\n",
    "A fully connected Nais-Net layer is defined as follows:\n",
    "\n",
    "$$\n",
    "x(k + 1) = x(k) + h\\sigma(Ax(k) + Bu + b) \\tag{49}\n",
    "$$\n",
    "\n",
    "where $x \\in \\mathbb{R}^n$ is the output of a layer, $u \\in \\mathbb{R}^m$ is the network input, $h > 0$, $A \\in \\mathbb{R}^{n \\times n}$ is the state transfer matrix, $B \\in \\mathbb{R}^{n \\times m}$ is the input transfer matrix, $b \\in \\mathbb{R}^n$ is the bias and $\\sigma$ is the activation function for each layer.\n",
    "\n",
    "We will be using an NVIDIA GeForce RTX 1080 GPU to run computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 Testing plan\n",
    "\n",
    "Given time limitations, the testing approach will be less than originally planned for. I hoped to test through FC, Resnet and Naisnet architectures, and a range of time steps and network depth. However, training time for each run was extremely long, sometimes over 10,000 seconds, which made wide testing unrealistic. The below represents what I was able to achieve:\n",
    "\n",
    "- **1D FC Sine**: We will start by looking at how closely a fully connected neural network, using a sine activation function, can approximate a vanilla call options with D=1, observing the loss function as the output.\n",
    "- **100 D Black-Schole Barenblatt**: We will then extend our testing of the vanilla call option to D=100, increasing the number of trajectories as well, and testing across activation functions (Sine, ReLU and Tanh) on NAIS-net and FC.\n",
    "- **Multi-Level Monte Carlo (MLMC)**: Introduced to reduce computational complexityby Giles [6], I look to see if a geometric and non-geometric MLMC can reduce time to train the neural networks.\n",
    "- **Correlation via Cholesky decomposition**: Introduce correlation between 2 assets we are modelling.\n",
    "\n",
    "## 7 Results\n",
    "\n",
    "### 7.1 Vanilla call option; D = 1\n",
    "\n",
    "Here, we start using the code kindly provided to us from the CQF institute, which helped greatly. The code allows us to run a neural network approximation of a vanilla call option value and compare it to the Black Scholes equation. The call option has parameters K = 1.0; r = 0.01; sigma = 0.25; q = 0 (assuming no dividend yield); T = 1.\n",
    "\n",
    "The neural net has been set to M = 1 (trajectories); N = 50 (time snamshots) and D = 1 (dimensions). We use an Adam optimiser to minimise the loss function.\n",
    "\n",
    "The model has been trained on 20k iterations with a learning rate of $1e^{-3}$ for the first 10k iterations and $1e^{-5}$ for the last 5k. Training times have been extremely long, and a major limiting factor in this project. For this test, we will use the sine activation function for a fully connected neural network and the Naisnet implementation. I have also included res-net in the functionality, but have omitted results as it, in general, had a lower performance when looking at the loss function.\n",
    "\n",
    "<img src=\"Figures\\1-dimensional_Call_Option_comp_FC_Sine.png\" alt=\"1-dimensional Call Option comp, FC-Sine\" width=\"500\">\n",
    "<img src=\"Figures\\1-dimensional_Call_Option_pred_FC_Sine.png\" alt=\"1-dimensional Call Option pred, FC-Sine\" width=\"500\">\n",
    "<img src=\"Figures\\1-dimensional_Call_Option_loss_FC_Sine.png\" alt=\"1-dimensional Call Option loss, FC-Sine\" width=\"500\">\n",
    "*Figure 4: FC-Sine activation; 1D Call option looking at the comparison between the NN approximation and the exact output from the Black-Scholes equation. We also look at a sample of 5 predicted outputs and the loss function across iterations. Training time 697s*\n",
    "\n",
    "### 7.2 Vanilla call option; D = 100  using Black-Scholes Barenblatt equation\n",
    "\n",
    "In order to look at dimensions for $D>1$ we need to consider a different approach to calculating the exact value of the call option price called the Black-Scholes Barenblatt equation\n",
    "\n",
    "Consider the following set of forward-backward stochastic differential equations:\n",
    "\n",
    "$$dX_t = \\text{diag}(X_t) \\, dW_t, \\quad t \\in [0, T], \\tag{50}$$ \n",
    "\n",
    "$$X_0 = \\xi,\\tag{51}$$\n",
    "\n",
    "$$dY_t = r(Y_t - Z_t' X_t) \\, dt + \\sigma Z_t' \\text{diag}(X_t) \\, dW_t, \\quad t \\in [0, T], \\tag{52}$$\n",
    "\n",
    "$$Y_T = g(X_T),\\tag{53}$$\n",
    "\n",
    "Here, we have the stochastic process defined by $X_t$ representing the underlying asset and $Y_t$ represents the value of the contract. We assume the underlying has no drift, i.e., $\\mu(t, X_t, Y_t, Z_t) = 0$, the volatility of the underlying is assumed to be constant $\\sigma$ with $\\sigma(t, X_t, Y_t) = \\sigma \\cdot \\text{diag}(X_t)$ and the drift function for the contract value $\\phi(t, X_t, Y_t, Z_t) = r(Y_t - Z_t' X_t)$. Furthermore, we assume that the price of the underlying assets is uncorrelated. Using the general solution for the set of equations in 3.1 given by the equation 3.2, we get the Black-Scholes Barenblatt partial differential equation:\n",
    "\n",
    "$$u_t = -\\frac{1}{2} \\text{Tr}[\\sigma^2 \\text{diag}(X^2) D^2u] + r(u - (Du)' x) \\tag{54}$$\n",
    "\n",
    "A solution to this equation is given by:\n",
    "\n",
    "$$u(t, x) = e^{(r + \\sigma^2)(T - t)} g(x) \\tag{55}$$\n",
    "\n",
    "**FC; Sine activation; 20k iterations with 0.01 learning rate; 5k iterations with 0.0001 learning rate (run times 1,903s to 2,691s)**\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_FC_Sine_1.png\" alt=\"1-dimensional Call Option comp, FC-Sine\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_FC_Sine_Loss1.png\" alt=\"1-dimensional Call Option pred, FC-Sine\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_FC_Sine_Errors1.png\" alt=\"1-dimensional Call Option loss, FC-Sine\" width=\"350\"></td>\n",
    "        <tr>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_FC_ReLU_.png\" alt=\"1-dimensional Call Option comp, Naisnet-ReLU\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_FC_ReLU_Loss.png\" alt=\"1-dimensional Call Option pred, Naisnet-ReLU\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_FC_ReLU_Errors.png\" alt=\"1-dimensional Call Option loss, Naisnet-ReLU\" width=\"350\"></td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "**Nais-net; Sine, ReLU and Tanh activation; 20k iterations with 0.01 learning rate; 5k iterations with 0.0001 learning rate**\n",
    "<table>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_Sine_.png\" alt=\"1-dimensional Call Option comp, Naisnet-Sine\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_Sine_Loss.png\" alt=\"1-dimensional Call Option pred, Naisnet-Sine\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_Sine_Errors.png\" alt=\"1-dimensional Call Option loss, Naisnet-Sine\" width=\"350\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_ReLU_1.png\" alt=\"1-dimensional Call Option comp, Naisnet-ReLU\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_ReLU_Loss1.png\" alt=\"1-dimensional Call Option pred, Naisnet-ReLU\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_ReLU_Errors1.png\" alt=\"1-dimensional Call Option loss, Naisnet-ReLU\" width=\"350\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_Tanh_1.png\" alt=\"1-dimensional Call Option comp, Naisnet-tanh\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_Tanh_Loss1.png\" alt=\"1-dimensional Call Option pred, Naisnet-tanh\" width=\"350\"></td>\n",
    "    <td><img src=\"Figures/BlackScholesBarenblatt100D_NaisNet_Tanh_Errors1.png\" alt=\"1-dimensional Call Option loss, Naisnet-tanh\" width=\"350\"></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results observed, we see a close match between the exact values from the Black Scholes Barenblatt formula and the neural network output. The Nais-net architecture produces better results based on the minimisation of training loss and relative error, with the Sine activation function being the best performing on aggregate. Additionally the training time for Sine activation was lower than the other activation functions, but training run times seemed to fluctuate, which was not expected.\n",
    "\n",
    "The difference between exact and learned paths is due to the stochastic nature of the process, which will introduce some randomness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Multi Level Monte Carlo\n",
    "\n",
    "Training times for the neural networks have been long, with time to run 60,000 iterations taking over 13,000s:\n",
    "\n",
    "<img src=\"Figures\\60k-100-dimensional_BSB_FC_Sine_training_loss.png\" alt=\"60k training run loss, FC-Sine\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing 250 iterations, with variations in both dimensions and time steps, it becomes evident that changes in the number of discretisation steps have a more pronounced impact on run time compared to changes in dimensions. This is attributed to the fact that the number of neural networks required increases with the number of time steps. Consequently, the time taken to train the model for 250 iterations grows exponentially with the number of discretisation steps. \n",
    "\n",
    "In contrast, since only the first layer of the model architecture is influenced by the dimensions of the underlying stochastic process, a significant increase in training time is observed only with a substantial increase in dimensions (notably, when dimensions increase from 100 to 500). Based on this observation, the focus of this section is to explore strategies to reduce the time associated with discretization steps.\n",
    "\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "  \n",
    "| Dimensions | Time   |\n",
    "|------------|--------|\n",
    "| 1          | 4.25s  |\n",
    "| 10         | 4.45s  |\n",
    "| 50         | 4.47s  |\n",
    "| 100        | 5.10s  |\n",
    "| 500        | 7.60s  |\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"flex: 1; padding: 10px;\">\n",
    "  \n",
    "| Time-Steps | Time    |\n",
    "|------------|---------|\n",
    "| 1          | 2.43s   |\n",
    "| 10         | 11.75s  |\n",
    "| 50         | 59.98s  |\n",
    "| 100        | 116.53s |\n",
    "| 500        | 584.07s |\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "The Multi-Level Monte Carlo (MLMC) method, introduced by Giles [6], is a technique aimed at reducing the computational complexity involved in estimating an expected value derived from a stochastic differential equation. In our context, this expected value corresponds to the approximation of $u(t, x)$, while the discretized version of the stochastic differential equations corresponds to the following.\n",
    "\n",
    "Consider the stochastic differential equation:\n",
    "\n",
    "$$\n",
    "dX_t = \\mu(t, X_t)dt + \\sigma(t, X_t)dW_t \\tag{56}\n",
    "$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ represent the drift and volatility terms, respectively, and $W$ is a Wiener process. Additionally, suppose there is a function $g$ that defines the terminal condition, i.e., $g(X_T)$ for a given terminal time $T$. An Euler discretization of the above SDE can be expressed as:\n",
    "\n",
    "$$\n",
    "X_{n+1} = X_n + \\mu(t, X_t)h + \\sigma(t, X_t)\\Delta W_t \\tag{57}\n",
    "$$\n",
    "\n",
    "MLMC is an approach to minimize the expected mean squared error of the estimates of $g(X_T)$ across $N$ simulations. This method significantly lowers the computational cost of achieving a specific accuracy level. Specifically, the computational cost can be reduced from $O(\\epsilon^{-3})$ to $O(\\epsilon^{-2}(log(\\epsilon))^2)$ by employing a multi-level strategy that decreases the variance while maintaining the Euler discretization's bias. This is accomplished by utilizing a sequence of different time-steps defined by:\n",
    "\n",
    "$$\n",
    "h = (L)^{-1}T \\quad \\text{for integers } L \\geq 2 \\tag{58}\n",
    "$$\n",
    "\n",
    "The strategy involves training the model with a smaller number of discretization time-steps, which results in lower initial accuracy but offers substantial reductions in computational cost, as shown in Table 3.2. The number of time-steps is then gradually increased at regular intervals to enhance the model's accuracy throughout the training process.\n",
    "\n",
    "#### 7.3.1 Geometric MLMC\n",
    "\n",
    "$$\n",
    "h = (L)^{-1}T \\quad \\text{for integers } L \\geq 2 \\tag{59}\n",
    "$$\n",
    "\n",
    "A geomteric implementation increases the number of time steps geometrically. In our example, we set L = 2 and N = 64, yielding time intervals of 2, 4, 8, 16, 32 and 64. We deploy a Nais-net architetcutre using a sine activation function that we have run previously and compare the time to train along with the loss function across training.\n",
    "\n",
    "**Geometric: Run time 1,348s vs 2,239s using traditional MC approach**\n",
    "\n",
    "<img src=\"Figures\\BSBGeometricMLMC100DPreds.png\" alt=\"GMLMC preds\" width=\"500\">\n",
    "<img src=\"Figures\\BSBGeometricMLMC100DLoss.png\" alt=\"60k training run loss, FC-Sine\" width=\"500\">\n",
    "\n",
    "We observe a large reduction in training time, but the loss and realtive error rate remain similar to the traditional Brownian motion/Monte Carlo implementation. If time allowed, I would like to investigate this further. What are the levels of discretisation that would materially imapct the performance, and what other factors is it dependent on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Correlation of assets\n",
    "\n",
    "In real-world scenarios, assets within a portfolio often exhibit measurable correlations with one another. We can account for the correlation of the underlying process $X_t$ by applying a Cholesky decomposition to the correlation matrix of these assets.\n",
    "\n",
    "Consider two Wiener processes $w_1$ and $w_2$ with a correlation coefficient $\\rho$. The correlation matrix for these processes can be decomposed into lower and upper triangular matrices such that:\n",
    "\n",
    "$$\n",
    "Corr(w_1, w_2) = (LL^T) = \\begin{pmatrix} 1 & 0 \\\\ \\rho & \\sqrt{1-\\rho^2} \\end{pmatrix} \\begin{pmatrix} 1 & \\rho \\\\ 0 & \\sqrt{1-\\rho^2} \\end{pmatrix} = \\begin{pmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{pmatrix} \\tag{60}\n",
    "$$\n",
    "\n",
    "This decomposition is referred to as the Cholesky decomposition. The correlation can be incorporated into the processes $w_1(t)$ and $w_2(t)$ using the Cholesky decomposition to generate correlated processes in $\\widetilde{W}_t$ as follows:\n",
    "\n",
    "$$\n",
    "\\widetilde{W}_t = \\begin{pmatrix} 1 & 0 \\\\ \\rho & \\sqrt{1-\\rho^2} \\end{pmatrix} \\begin{pmatrix} w_1(t) \\\\ w_2(t) \\end{pmatrix} = \\begin{pmatrix} w_1(t) \\\\ \\rho w_1(t) + \\sqrt{1-\\rho^2} w_2(t) \\end{pmatrix} \\tag{61}\n",
    "$$\n",
    "\n",
    "Given that the process defined by $X_t$ follows a Wiener process, we can extend this concept to the equations in Section 3.6 when the assets represented in $X_t$ are correlated. Consider $D$ assets with the following correlation matrix, where $\\rho_{ij}$ denotes the correlation between asset $i$ and asset $j$:\n",
    "\n",
    "$$\n",
    "Corr(x_1, x_2, ...x_D) = \n",
    "\\begin{pmatrix}\n",
    "    1 & \\rho_{12} & \\rho_{13} & \\dots & \\rho_{1,D-1} & \\rho_{1D} \\\\\n",
    "    \\rho_{21} & 1 & \\rho_{23} & \\dots & \\rho_{2,D-1} & \\rho_{2D} \\\\\n",
    "    \\rho_{31} & \\rho_{32} & 1 & \\dots & \\rho_{3,D-1} & \\rho_{3D} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
    "    \\rho_{D-1,1} & \\rho_{D-1,2} & \\rho_{D-1,3} & \\dots & 1 & \\rho_{D-1,D} \\\\\n",
    "    \\rho_{D1} & \\rho_{D2} & \\rho_{D3} & \\dots & \\rho_{D,D-1} & 1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\tag{62}$$\n",
    "\n",
    "Applying the Cholesky decomposition to yield $Corr(x_1, x_2, ...x_D) = (LL^T)$, the equations in Section 3.6 can be rewritten as:\n",
    "\n",
    "$$\n",
    "dX_t = \\sigma \\text{diag}(X_t) L dW_t, \\quad t \\in [0, T], \\tag{63}\n",
    "$$\n",
    "\n",
    "$$\n",
    "X_0 = \\xi, \\tag{64}\n",
    "$$\n",
    "\n",
    "$$\n",
    "dY_t = r(Y_t - Z_t'X_t) dt + \\sigma Z_t' \\text{diag}(X_t) L dW_t, \\quad t \\in [0, T], \\tag{65}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Y_T = g(X_T). \\tag{66}\n",
    "$$\n",
    "\n",
    "We discretise the above set of equations using the Euler-Maruyama scheme, and use our model to approximate $Y_t$, similar to how we did for the uncorrelated scenario. The figures below illustrate the loss function's progression and the model's predictions for correlated underlying asset prices.\n",
    "\n",
    "<img src=\"Figures\\2D_corr_naisnet_sine.png\" alt=\"2D corr\" width=\"500\">\n",
    "<img src=\"Figures\\2D_corr_naisnet_sine_loss.png\" alt=\"2D corr training run loss, FC-Sine\" width=\"500\">\n",
    "<img src=\"Figures\\2D_corr_naisnet_sine_error.png\" alt=\"2D corr mean and error, FC-Sine\" width=\"500\">\n",
    "\n",
    "We have successfully introduced asset correlation, which brings this one step closer to a real world application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 Conclusion\n",
    "\n",
    "In this project, we explored the application of Deep Neural Networks (DNNs) to solve high-dimensional Partial Differential Equations (PDEs), specifically focusing on financial models governed by stochastic differential equations. Through various network architectures, including Fully Connected Networks and Nonlinear Activation Identity Scalability Networks (NAIS-Net), we demonstrated the potential of deep learning in overcoming the computational challenges associated with high-dimensional PDEs.\n",
    "\n",
    "Our approach successfully approximated the solutions to these complex equations, leveraging the strengths of each network architecture to enhance stability and accuracy. The Fully Connected Networks provided a versatile baseline, while NAIS-net addressed issues of vanishing gradients in deeper networks. NAIS-Net, with its innovative design, offered additional stability, making it particularly effective in scenarios with correlated underlying assets.\n",
    "\n",
    "Unfortunately, the project involved heavy computation, and with limited time, I was unable to delve into many different avenues of investigation. The MLMC approach was extremely interesting, and reduced training time significantly. I wish to also explore how the NN performance relates to the number of discrete time steps and paths that are used to generate. Additionally, I would have wanted to explore different options structures (look back and Asian).\n",
    "\n",
    "\n",
    "\n",
    "##### Aside\n",
    "\n",
    "This project was significantly outside of my comfort zone, but represented an almost perfect combination of what was taught over the CQF program, covering quantitative finance and AI. I have learned so much over the course of this project and increased my coding efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 References\n",
    "[1] Towards Robust and Stable Deep Learning Algorithms for Forward Backward Stochastic Differential Equations\n",
    "Batuhan Güler, Alexis Laignelet, Panos Parpas, 2019\n",
    "https://arxiv.org/pdf/1910.11623\n",
    "\n",
    "[2] Maziar Raissi. Forward-backward stochastic neural networks: Deep learning of high-dimensional partial differential equations, 2018.\n",
    "\n",
    "[3] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.\n",
    "\n",
    "[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015.\n",
    "\n",
    "[5] Marco Ciccone, Marco Gallieri, Jonathan Masci, Christian Osendorfer, and Faustino Gomez. Nais-net: Stable deep networks from non-autonomous differential equations, 2021\n",
    "\n",
    "[6] Michael B. Giles. Multilevel monte carlo path simulation. Operations Research,56(3):607–617, 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 Appendix (code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Models.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    \"\"\"This class defines the sine activation function as a nn.Module\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Sine, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "\n",
    "\n",
    "class Naisnet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, stable, activation):\n",
    "        super(Naisnet, self).__init__()\n",
    "\n",
    "        self.layers = layers\n",
    "        self.layer1 = nn.Linear(in_features=layers[0], out_features=layers[1])\n",
    "        self.layer2 = nn.Linear(in_features=layers[1], out_features=layers[2])\n",
    "        self.layer2_input = nn.Linear(in_features=layers[0], out_features=layers[2])\n",
    "        self.layer3 = nn.Linear(in_features=layers[2], out_features=layers[3])\n",
    "        if len(layers) == 5:\n",
    "            self.layer3_input = nn.Linear(in_features=layers[0], out_features=layers[3])\n",
    "            self.layer4 = nn.Linear(in_features=layers[3], out_features=layers[4])\n",
    "        elif len(layers) == 6:\n",
    "            self.layer3_input = nn.Linear(in_features=layers[0], out_features=layers[3])\n",
    "            self.layer4 = nn.Linear(in_features=layers[3], out_features=layers[4])\n",
    "            self.layer4_input = nn.Linear(in_features=layers[0], out_features=layers[4])\n",
    "            self.layer5 = nn.Linear(in_features=layers[4], out_features=layers[5])\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.epsilon = 0.01\n",
    "        self.stable = stable\n",
    "\n",
    "    def project(self, layer, out):  # Building block for the NAIS-Net\n",
    "        weights = layer.weight\n",
    "        delta = 1 - 2 * self.epsilon\n",
    "        RtR = torch.matmul(weights.t(), weights)\n",
    "        norm = torch.norm(RtR)\n",
    "        if norm > delta:\n",
    "            RtR = delta ** (1 / 2) * RtR / (norm ** (1 / 2))\n",
    "        A = RtR + torch.eye(RtR.shape[0]).cuda() * self.epsilon\n",
    "\n",
    "        return F.linear(out, -A, layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        shortcut = out\n",
    "        if self.stable:\n",
    "            out = self.project(self.layer2, out)\n",
    "            out = out + self.layer2_input(u)\n",
    "        else:\n",
    "            out = self.layer2(out)\n",
    "        out = self.activation(out)\n",
    "        out = out + shortcut\n",
    "\n",
    "        if len(self.layers) == 4:\n",
    "            out = self.layer3(out)\n",
    "            return out\n",
    "\n",
    "        if len(self.layers) == 5:\n",
    "            shortcut = out\n",
    "            if self.stable:\n",
    "                out = self.project(self.layer3, out)\n",
    "                out = out + self.layer3_input(u)\n",
    "            else:\n",
    "                out = self.layer3(out)\n",
    "            out = self.activation(out)\n",
    "            out = out + shortcut\n",
    "\n",
    "            out = self.layer4(out)\n",
    "            return out\n",
    "        \n",
    "        if len(self.layers) == 6:\n",
    "            shortcut = out\n",
    "            if self.stable:\n",
    "                out = self.project(self.layer3, out)\n",
    "                out = out + self.layer3_input(u)\n",
    "            else:\n",
    "                out = self.layer3(out)\n",
    "            out = self.activation(out)\n",
    "            out = out + shortcut\n",
    "\n",
    "\n",
    "            shortcut = out\n",
    "            if self.stable:\n",
    "                out = self.project(self.layer4, out)\n",
    "                out = out + self.layer4_input(u)\n",
    "            else:\n",
    "                out = self.layer4(out)\n",
    "\n",
    "            out = self.activation(out)\n",
    "            out = out + shortcut\n",
    "\n",
    "            out = self.layer5(out)\n",
    "            return out\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, stable, activation):\n",
    "        super(Resnet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_features=layers[0], out_features=layers[1])\n",
    "        self.layer2 = nn.Linear(in_features=layers[1], out_features=layers[2])\n",
    "        self.layer2_input = nn.Linear(\n",
    "            in_features=layers[0], out_features=layers[2])\n",
    "        self.layer3 = nn.Linear(in_features=layers[2], out_features=layers[3])\n",
    "        self.layer3_input = nn.Linear(\n",
    "            in_features=layers[0], out_features=layers[3])\n",
    "        self.layer4 = nn.Linear(in_features=layers[3], out_features=layers[4])\n",
    "        self.layer4_input = nn.Linear(\n",
    "            in_features=layers[0], out_features=layers[4])\n",
    "        self.layer5 = nn.Linear(in_features=layers[4], out_features=layers[5])\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.epsilon = 0.01\n",
    "        self.stable = stable\n",
    "\n",
    "    def project(self, layer, out):  # Building block for the NAIS-Net\n",
    "        weights = layer.weight\n",
    "        delta = 1 - 2 * self.epsilon\n",
    "        RtR = torch.matmul(weights.t(), weights)\n",
    "        norm = torch.norm(RtR)\n",
    "        if norm > delta:\n",
    "            RtR = delta ** (1 / 2) * RtR / (norm ** (1 / 2))\n",
    "        A = RtR + torch.eye(RtR.shape[0]).cuda() * self.epsilon\n",
    "\n",
    "        return F.linear(out, -A, layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        shortcut = out\n",
    "        if self.stable:\n",
    "            out = self.project(self.layer2, out)\n",
    "            out += self.layer2_input(u)\n",
    "        else:\n",
    "            out = self.layer2(out)\n",
    "        out = self.activation(out)\n",
    "        out += shortcut\n",
    "\n",
    "        shortcut = out\n",
    "        if self.stable:\n",
    "            out = self.project(self.layer3, out)\n",
    "            out += self.layer3_input(u)\n",
    "        else:\n",
    "            out = self.layer3(out)\n",
    "        out = self.activation(out)\n",
    "        out += shortcut\n",
    "\n",
    "        shortcut = out\n",
    "        if self.stable:\n",
    "            out = self.project(self.layer4, out)\n",
    "            out += self.layer4_input(u)\n",
    "        else:\n",
    "            out = self.layer4(out)\n",
    "\n",
    "        out = self.activation(out)\n",
    "        out += shortcut\n",
    "\n",
    "        out = self.layer5(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FBSNNs.py\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from Models import *\n",
    "\n",
    "\n",
    "class FBSNN(ABC):\n",
    "    def __init__(self, Xi, T, M, N, D, Mm, layers, mode, activation):\n",
    "        # Constructor for the FBSNN class\n",
    "        # Initializes the neural network with specified parameters and architecture\n",
    "        \n",
    "        # Parameters:\n",
    "        # Xi: Initial condition (numpy array) for the stochastic process\n",
    "        # T: Terminal time\n",
    "        # M: Number of trajectories (batch size)\n",
    "        # N: Number of time snapshots\n",
    "        # D: Number of dimensions for the problem\n",
    "        # Mm: Number of discretization points for the SDE\n",
    "        # layers: List indicating the size of each layer in the neural network\n",
    "        # mode: Specifies the architecture of the neural network (e.g., 'FC' for fully connected)\n",
    "        # activation: Activation function to be used in the neural network\n",
    "\n",
    "        # Check if CUDA is available and set the appropriate device (GPU or CPU)\n",
    "        device_idx = 0\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        # Initialize the initial condition, convert it to a PyTorch tensor, and send to the device\n",
    "        self.Xi = torch.from_numpy(Xi).float().to(self.device)  # initial point\n",
    "        self.Xi.requires_grad = True\n",
    "\n",
    "        # Store other parameters as attributes of the class.\n",
    "        self.T = T  # terminal time\n",
    "        self.M = M  # number of trajectories\n",
    "        self.N = N  # number of time snapshots\n",
    "        self.D = D  # number of dimensions\n",
    "        self.Mm = Mm  # number of discretization points for the SDE\n",
    "        self.strike = 1.0 * self.D  # strike price\n",
    "\n",
    "        self.mode = mode  # architecture of the neural network\n",
    "        self.activation = activation  # activation function        # Initialize the activation function based on the provided parameter\n",
    "        if activation == \"Sine\":\n",
    "            self.activation_function = Sine()\n",
    "        elif activation == \"ReLU\":\n",
    "            self.activation_function = nn.ReLU()\n",
    "        elif activation == \"Tanh\":\n",
    "            self.activation_function = nn.Tanh()\n",
    "\n",
    "        # Initialize the neural network based on the chosen mode\n",
    "        if self.mode == \"FC\":\n",
    "            # Fully Connected architecture\n",
    "            self.layers = []\n",
    "            for i in range(len(layers) - 2):\n",
    "                self.layers.append(nn.Linear(in_features=layers[i], out_features=layers[i + 1]))\n",
    "                self.layers.append(self.activation_function)\n",
    "            self.layers.append(nn.Linear(in_features=layers[-2], out_features=layers[-1]))\n",
    "            self.model = nn.Sequential(*self.layers).to(self.device)\n",
    "\n",
    "        elif self.mode == \"Naisnet\":\n",
    "            # NAIS-Net architecture\n",
    "            self.model = Naisnet(layers, stable=True, activation=self.activation_function).to(self.device)\n",
    "        \n",
    "        elif self.mode == \"Resnet\":\n",
    "            # Resnet architecture\n",
    "            self.model = Resnet(layers, stable=True, activation=self.activation_function).to(self.device)\n",
    "\n",
    "        # Apply a custom weights initialization to the model.\n",
    "        self.model.apply(self.weights_init)\n",
    "\n",
    "        # Initialize lists to record training loss and iterations.\n",
    "        self.training_loss = []\n",
    "        self.iteration = []\n",
    "\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        # Custom weight initialization method for neural network layers\n",
    "        # Parameters:\n",
    "        # m: A layer of the neural network\n",
    "\n",
    "        if type(m) == nn.Linear:\n",
    "            # Initialize the weights of the linear layer using Xavier uniform initialization\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    def net_u(self, t, X):  # M x 1, M x D\n",
    "        # Computes the output of the neural network and its gradient with respect to the input state X\n",
    "        # Parameters:\n",
    "        # t: A batch of time instances, with dimensions M x 1\n",
    "        # X: A batch of state variables, with dimensions M x D\n",
    "\n",
    "        # Concatenate the time and state variables along second dimension\n",
    "        # to form the input for the neural network\n",
    "        input = torch.cat((t, X), 1)  \n",
    "\n",
    "        # Pass the concatenated input through the neural network model\n",
    "        # The output u is a tensor of dimensions M x 1, representing the value function at each input (t, X)\n",
    "        u = self.model(input)  # M x 1\n",
    "\n",
    "        # Compute the gradient of the output u with respect to the state variables X\n",
    "        # The gradient is calculated for each input in the batch, resulting in a tensor of dimensions M x D\n",
    "        Du = torch.autograd.grad(outputs=[u], inputs=[X], grad_outputs=torch.ones_like(u), \n",
    "                                allow_unused=True, retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        return u, Du\n",
    "\n",
    "    def Dg_tf(self, X):  # M x D\n",
    "        # Calculates the gradient of the function g with respect to the input X\n",
    "        # Parameters:\n",
    "        # X: A batch of state variables, with dimensions M x D\n",
    "\n",
    "        g = self.g_tf(X)  # M x 1\n",
    "\n",
    "        # Now, compute the gradient of g with respect to X\n",
    "        # The gradient is calculated for each input in the batch, resulting in a tensor of dimensions M x D\n",
    "        Dg = torch.autograd.grad(outputs=[g], inputs=[X], grad_outputs=torch.ones_like(g), \n",
    "                                allow_unused=True, retain_graph=True, create_graph=True)[0] \n",
    "\n",
    "        return Dg\n",
    "\n",
    "\n",
    "    def loss_function(self, t, W, Xi):\n",
    "        # Calculates the loss for the neural network\n",
    "        # Parameters:\n",
    "        # t: A batch of time instances, with dimensions M x (N+1) x 1\n",
    "        # W: A batch of Brownian motion increments, with dimensions M x (N+1) x D\n",
    "        # Xi: Initial state, with dimensions 1 x D\n",
    "\n",
    "        loss = 0  # Initialize the loss to zero.\n",
    "        X_list = []  # List to store the states at each time step.\n",
    "        Y_list = []  # List to store the network outputs at each time step.\n",
    "\n",
    "        # Initial time and Brownian motion increment.\n",
    "        t0 = t[:, 0, :]\n",
    "        W0 = W[:, 0, :]\n",
    "\n",
    "        # Initial state for all trajectories\n",
    "        X0 = Xi.repeat(self.M, 1).view(self.M, self.D)  # M x D\n",
    "        Y0, Z0 = self.net_u(t0, X0)  # Obtain the network output and its gradient at the initial state\n",
    "\n",
    "        # Store the initial state and the network output\n",
    "        X_list.append(X0)\n",
    "        Y_list.append(Y0)\n",
    "\n",
    "        # Iterate over each time step\n",
    "        for n in range(0, self.N):\n",
    "            # Next time step and Brownian motion increment\n",
    "            t1 = t[:, n + 1, :]\n",
    "            W1 = W[:, n + 1, :]\n",
    "            # Compute the next state using the Euler-Maruyama method\n",
    "            X1 = X0 + self.mu_tf(t0, X0, Y0, Z0) * (t1 - t0) + torch.squeeze(\n",
    "                torch.matmul(self.sigma_tf(t0, X0, Y0), (W1 - W0).unsqueeze(-1)), dim=-1)\n",
    "            \n",
    "            # Compute the predicted value (Y1_tilde) at the next state\n",
    "            Y1_tilde = Y0 + self.phi_tf(t0, X0, Y0, Z0) * (t1 - t0) + torch.sum(\n",
    "                Z0 * torch.squeeze(torch.matmul(self.sigma_tf(t0, X0, Y0), (W1 - W0).unsqueeze(-1))), dim=1,\n",
    "                keepdim=True)\n",
    "            \n",
    "            # Obtain the network output and its gradient at the next state\n",
    "            Y1, Z1 = self.net_u(t1, X1)\n",
    "            # Add the squared difference between Y1 and Y1_tilde to the loss\n",
    "            loss += torch.sum(torch.pow(Y1 - Y1_tilde, 2))\n",
    "\n",
    "            # Update the variables for the next iteration\n",
    "            t0, W0, X0, Y0, Z0 = t1, W1, X1, Y1, Z1\n",
    "\n",
    "            # Store the current state and the network output\n",
    "            X_list.append(X0)\n",
    "            Y_list.append(Y0)\n",
    "\n",
    "        # Add the terminal condition to the loss: \n",
    "        # the difference between the network output and the target at the final state\n",
    "        loss += torch.sum(torch.pow(Y1 - self.g_tf(X1), 2))\n",
    "        # Add the difference between the network's gradient and the gradient of g at the final state\n",
    "        loss += torch.sum(torch.pow(Z1 - self.Dg_tf(X1), 2))\n",
    "\n",
    "        # Stack the states and network outputs for all time steps\n",
    "        X = torch.stack(X_list, dim=1)\n",
    "        Y = torch.stack(Y_list, dim=1)\n",
    "\n",
    "        # Return the loss and the states and outputs at each time step\n",
    "        # The final element returned is the first element of the network output, for reference or further use\n",
    "        return loss, X, Y, Y[0, 0, 0]\n",
    "\n",
    "\n",
    "    def fetch_minibatch(self):  # Generate time + a Brownian motion\n",
    "        # Generates a minibatch of time steps and corresponding Brownian motion paths\n",
    "\n",
    "        T = self.T  # Terminal time\n",
    "        M = self.M  # Number of trajectories (batch size)\n",
    "        N = self.N  # Number of time snapshots\n",
    "        D = self.D  # Number of dimensions\n",
    "\n",
    "        # Initialize arrays for time steps and Brownian increments\n",
    "        Dt = np.zeros((M, N + 1, 1))  # Time step sizes for each trajectory and time snapshot\n",
    "        DW = np.zeros((M, N + 1, D))  # Brownian increments for each trajectory, time snapshot, and dimension\n",
    "\n",
    "        # Calculate the time step size\n",
    "        dt = T / N\n",
    "\n",
    "        # Populate the time step sizes for each trajectory and time snapshot (excluding the initial time)\n",
    "        Dt[:, 1:, :] = dt\n",
    "        # Generate Brownian increments for each trajectory and time snapshot\n",
    "        DW_uncorrelated = np.sqrt(dt) * np.random.normal(size=(M, N, D))\n",
    "        DW[:, 1:, :] = DW_uncorrelated # np.einsum('ij,mnj->mni', self.L, DW_uncorrelated) # Apply Cholesky matrix to introduce correlations\n",
    "\n",
    "        # Cumulatively sum the time steps and Brownian increments to get the actual time values and Brownian paths\n",
    "        t = np.cumsum(Dt, axis=1)  # Cumulative time for each trajectory and time snapshot\n",
    "        W = np.cumsum(DW, axis=1)  # Cumulative Brownian motion for each trajectory, time snapshot, and dimension\n",
    "\n",
    "        # Convert the numpy arrays to PyTorch tensors and transfer them to the configured device (CPU or GPU)\n",
    "        t = torch.from_numpy(t).float().to(self.device)\n",
    "        W = torch.from_numpy(W).float().to(self.device)\n",
    "\n",
    "        # Return the time values and Brownian paths.\n",
    "        return t, W\n",
    "\n",
    "    def train(self, N_Iter, learning_rate):\n",
    "        # Train the neural network model.\n",
    "        # Parameters:\n",
    "        # N_Iter: Number of iterations for the training process\n",
    "        # learning_rate: Learning rate for the optimizer\n",
    "\n",
    "        # Initialize an array to store temporary loss values for averaging\n",
    "        loss_temp = np.array([])\n",
    "\n",
    "        # Check if there are previous iterations and set the starting iteration number\n",
    "        previous_it = 0\n",
    "        if self.iteration != []:\n",
    "            previous_it = self.iteration[-1]\n",
    "\n",
    "        # Set up the optimizer (Adam) for the neural network with the specified learning rate\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        # Record the start time for timing the training process\n",
    "        start_time = time.time()\n",
    "        # Training loop\n",
    "        for it in range(previous_it, previous_it + N_Iter):\n",
    "            \n",
    "            if it >= 4000 and it < 20000:\n",
    "                self.N = int(np.ceil(self.Mm ** (int(it / 4000) + 1)))\n",
    "            elif it < 4000:\n",
    "                self.N = int(np.ceil(self.Mm))\n",
    "\n",
    "            # Zero the gradients before each iteration\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Fetch a minibatch of time steps and Brownian motion paths\n",
    "            t_batch, W_batch = self.fetch_minibatch()  # M x (N+1) x 1, M x (N+1) x D\n",
    "\n",
    "            # Compute the loss for the current batch\n",
    "            loss, X_pred, Y_pred, Y0_pred = self.loss_function(t_batch, W_batch, self.Xi)\n",
    "            # Perform backpropagation\n",
    "            self.optimizer.zero_grad()  # Zero the gradients again to ensure correct gradient accumulation\n",
    "            loss.backward()  # Compute the gradients of the loss w.r.t. the network parameters\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()  # Update the network parameters based on the gradients\n",
    "\n",
    "            # Store the current loss value for later averaging\n",
    "            loss_temp = np.append(loss_temp, loss.cpu().detach().numpy())\n",
    "            # Print the training progress every 100 iterations\n",
    "            if it % 100 == 0:\n",
    "                elapsed = time.time() - start_time  # Calculate the elapsed time\n",
    "                print('It: %d, Loss: %.3e, Y0: %.3f, Time: %.2f, Learning Rate: %.3e' %\n",
    "                    (it, loss, Y0_pred, elapsed, learning_rate))\n",
    "                start_time = time.time()  # Reset the start time for the next print interval\n",
    "\n",
    "            # Record the average loss and iteration number every 100 iterations\n",
    "            if it % 100 == 0:\n",
    "                self.training_loss.append(loss_temp.mean())  # Append the average loss\n",
    "                loss_temp = np.array([])  # Reset the temporary loss array\n",
    "                self.iteration.append(it)  # Append the current iteration number\n",
    "        # Stack the iteration and training loss for plotting\n",
    "        graph = np.stack((self.iteration, self.training_loss))\n",
    "\n",
    "        # Return the training history (iterations and corresponding losses)\n",
    "        return graph\n",
    "\n",
    "    def predict(self, Xi_star, t_star, W_star):\n",
    "        # Predicts the output of the neural network\n",
    "        # Parameters:\n",
    "        # Xi_star: The initial state for the prediction, given as a numpy array\n",
    "        # t_star: The time steps at which predictions are to be made\n",
    "        # W_star: The Brownian motion paths corresponding to the time steps\n",
    "\n",
    "        # Convert the initial state (Xi_star) from a numpy array to a PyTorch tensor\n",
    "        Xi_star = torch.from_numpy(Xi_star).float().to(self.device)\n",
    "        Xi_star.requires_grad = True\n",
    "\n",
    "        # Compute the loss and obtain predicted states (X_star) and outputs (Y_star) using the trained model\n",
    "        _, X_star, Y_star, _ = self.loss_function(t_star, W_star, Xi_star)\n",
    "\n",
    "        # Return the predicted states and outputs\n",
    "        # These predictions correspond to the neural network's estimation of the state and output at each time step\n",
    "        return X_star, Y_star\n",
    "\n",
    "    def save_model(self, file_name):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'training_loss': self.training_loss,\n",
    "            'iteration': self.iteration\n",
    "        }, file_name)\n",
    "    \n",
    "    def load_model(self, file_name):\n",
    "        checkpoint = torch.load(file_name, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.training_loss = checkpoint['training_loss']\n",
    "        self.iteration = checkpoint['iteration']\n",
    "\n",
    "    @abstractmethod\n",
    "    def phi_tf(self, t, X, Y, Z):  # M x 1, M x D, M x 1, M x D\n",
    "        # Abstract method for defining the drift term in the SDE\n",
    "        # Parameters:\n",
    "        # t: Time instances, size M x 1\n",
    "        # X: State variables, size M x D\n",
    "        # Y: Function values at state variables, size M x 1\n",
    "        # Z: Gradient of the function with respect to state variables, size M x D\n",
    "        # Expected return size: M x 1\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def g_tf(self, X):  # M x D\n",
    "        # Abstract method for defining the terminal condition of the SDE\n",
    "        # Parameter:\n",
    "        # X: Terminal state variables, size M x D\n",
    "        # Expected return size: M x 1\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def mu_tf(self, t, X, Y, Z):  # M x 1, M x D, M x 1, M x D\n",
    "        # Abstract method for defining the drift coefficient of the underlying stochastic process\n",
    "        # Parameters:\n",
    "        # t: Time instances, size M x 1\n",
    "        # X: State variables, size M x D\n",
    "        # Y: Function values at state variables, size M x 1\n",
    "        # Z: Gradient of the function with respect to state variables, size M x D\n",
    "        # Default implementation returns a zero tensor of size M x D\n",
    "        M = self.M\n",
    "        D = self.D\n",
    "        return torch.zeros([M, D]).to(self.device)  # M x D\n",
    "\n",
    "    @abstractmethod\n",
    "    def sigma_tf(self, t, X, Y):  # M x 1, M x D, M x 1\n",
    "        # Abstract method for defining the diffusion coefficient of the underlying stochastic process\n",
    "        # Parameters:\n",
    "        # t: Time instances, size M x 1\n",
    "        # X: State variables, size M x D\n",
    "        # Y: Function values at state variables, size M x 1\n",
    "        # Default implementation returns a diagonal matrix of ones of size M x D x D\n",
    "        M = self.M\n",
    "        D = self.D\n",
    "        return torch.diag_embed(torch.ones([M, D])).to(self.device)  # M x D x D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#CallOption.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from FBSNNs import FBSNN\n",
    "\n",
    "\n",
    "class CallOption(FBSNN):\n",
    "    def __init__(self, Xi, T, M, N, D, Mm, layers, mode, activation):\n",
    "        # Constructor for the BlackScholesBarenblatt class\n",
    "        # Initializes a new instance with specified parameters for the neural network\n",
    "        # Inherits from FBSNN (Forward-Backward Stochastic Neural Network)\n",
    "        # Parameters:\n",
    "        # Xi: Initial condition\n",
    "        # T: Time horizon\n",
    "        # M: Batch size\n",
    "        # N: Number of time discretization steps\n",
    "        # D: Dimension of the problem\n",
    "        # layers: Configuration of the neural network layers\n",
    "        # mode: Operation mode\n",
    "        # activation: Activation function for the neural network\n",
    "        super().__init__(Xi, T, M, N, D, Mm, layers, mode, activation)\n",
    "\n",
    "    def phi_tf(self, t, X, Y, Z):\n",
    "        # Defines the drift term in the Black-Scholes-Barenblatt equation for a batch\n",
    "        # t: Batch of current times, size M x 1\n",
    "        # X: Batch of current states, size M x D\n",
    "        # Y: Batch of current value functions, size M x 1\n",
    "        # Z: Batch of gradients of the value function with respect to X, size M x D\n",
    "        # Returns the drift term for each instance in the batch, size M x 1\n",
    "        rate = 0.01  # Risk-free interest rate\n",
    "        return rate * (Y)  # M x 1\n",
    "\n",
    "    def g_tf(self, X):\n",
    "        # Terminal condition for the Black-Scholes-Barenblatt equation for a batch\n",
    "        # X: Batch of terminal states, size M x D\n",
    "        # Returns the terminal condition for each instance in the batch, size M x 1\n",
    "        temp = torch.sum(X, dim=1, keepdim=True)\n",
    "        return torch.maximum(temp - self.strike, torch.tensor(0.0))\n",
    "\n",
    "    def mu_tf(self, t, X, Y, Z):\n",
    "        # Drift coefficient of the underlying stochastic process for a batch\n",
    "        # Inherits from the superclass FBSNN without modification\n",
    "        # Parameters are the same as in phi_tf, with batch sizes\n",
    "        rate = 0.01\n",
    "        return rate * X  # M x D\n",
    "\n",
    "    def sigma_tf(self, t, X, Y):\n",
    "        # Diffusion coefficient of the underlying stochastic process for a batch\n",
    "        # t: Batch of current times, size M x 1\n",
    "        # X: Batch of current states, size M x D\n",
    "        # Y: Batch of current value functions, size M x 1 (not used in this method)\n",
    "        # Returns a batch of diagonal matrices, each of size D x D, for the diffusion coefficients\n",
    "        # Each matrix is scaled by 0.4 times the corresponding state in X\n",
    "        sigma = 0.25  # Volatility\n",
    "        return sigma * torch.diag_embed(X)  # M x D x D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BlackScholesBarenblatt.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from FBSNNs import FBSNN\n",
    "\n",
    "\n",
    "class BlackScholesBarenblatt(FBSNN):\n",
    "    def __init__(self, Xi, T, M, N, D, Mm, layers, mode, activation):\n",
    "        # Constructor for the BlackScholesBarenblatt class\n",
    "        # Initializes a new instance with specified parameters for the neural network\n",
    "        # Inherits from FBSNN (Forward-Backward Stochastic Neural Network)\n",
    "        # Parameters:\n",
    "        # Xi: Initial condition\n",
    "        # T: Time horizon\n",
    "        # M: Batch size\n",
    "        # N: Number of time discretization steps\n",
    "        # D: Dimension of the problem\n",
    "        # Mm: Number of discretization points for the SDE\n",
    "        # layers: Configuration of the neural network layers\n",
    "        # mode: Operation mode\n",
    "        # activation: Activation function for the neural network\n",
    "        super().__init__(Xi, T, M, N, D, Mm, layers, mode, activation)\n",
    "\n",
    "    def phi_tf(self, t, X, Y, Z):\n",
    "        # Defines the drift term in the Black-Scholes-Barenblatt equation for a batch\n",
    "        # t: Batch of current times, size M x 1\n",
    "        # X: Batch of current states, size M x D\n",
    "        # Y: Batch of current value functions, size M x 1\n",
    "        # Z: Batch of gradients of the value function with respect to X, size M x D\n",
    "        # Returns the drift term for each instance in the batch, size M x 1\n",
    "        return 0.05 * (Y - torch.sum(X * Z, dim=1, keepdim=True))  # M x 1\n",
    "\n",
    "    def g_tf(self, X):\n",
    "        # Terminal condition for the Black-Scholes-Barenblatt equation for a batch\n",
    "        # X: Batch of terminal states, size M x D\n",
    "        # Returns the terminal condition for each instance in the batch, size M x 1\n",
    "        return torch.sum(X ** 2, 1, keepdim=True)  # M x 1\n",
    "\n",
    "    def mu_tf(self, t, X, Y, Z):\n",
    "        # Drift coefficient of the underlying stochastic process for a batch\n",
    "        # Inherits from the superclass FBSNN without modification\n",
    "        # Parameters are the same as in phi_tf, with batch sizes\n",
    "        return super().mu_tf(t, X, Y, Z)  # M x D\n",
    "\n",
    "    def sigma_tf(self, t, X, Y):\n",
    "        # Diffusion coefficient of the underlying stochastic process for a batch\n",
    "        # t: Batch of current times, size M x 1\n",
    "        # X: Batch of current states, size M x D\n",
    "        # Y: Batch of current value functions, size M x 1 (not used in this method)\n",
    "        # Returns a batch of diagonal matrices, each of size D x D, for the diffusion coefficients\n",
    "        # Each matrix is scaled by 0.4 times the corresponding state in X\n",
    "        return 0.4 * torch.diag_embed(X)  # M x D x D\n",
    "\n",
    "\n",
    "def u_exact(T, t, X):\n",
    "    # Calculates the exact solution for the Black Scholes Barenblatt equation\n",
    "    # Parameters:\n",
    "    # T: The terminal time, a vector of size (N+1) x 1. Represents the final time in the time discretization\n",
    "    # t: The current time, a vector of size (N+1) x 1. Represents the time steps in the time discretization\n",
    "    # X: The current state, an array of size (N+1) x D. Represents the state variables at each time step\n",
    "\n",
    "    r = 0.05         # Represents the risk-free interest rate\n",
    "    sigma_max = 0.4  # Represents the maximum volatility\n",
    "\n",
    "    # The exact solution is calculated using an exponential term and a summation term\n",
    "    # The exponential term accounts for the time value of money and volatility\n",
    "    # The summation term represents the square of the state variables summed across the D dimensions\n",
    "    # The solution is computed for each time step and state, resulting in a vector of size (N+1) x 1\n",
    "    # (N+1) x 1\n",
    "    return np.exp((r + sigma_max ** 2) * (T - t)) * np.sum(X ** 2, 1, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#vanilla_call.py\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"Algorithms/\"))\n",
    "sys.path.append(os.path.abspath(\"models/\"))\n",
    "#%%\n",
    "from FBSNNs import *\n",
    "from CallOption import *\n",
    "#%%\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "M = 1 # number of trajectories (batch size)\n",
    "N = 50  # number of time snapshots\n",
    "D = 1 # number of dimensions\n",
    "Mm = N ** (1/5)\n",
    "\n",
    "layers = [D + 1] + 4 * [256] + [1]\n",
    "\n",
    "Xi = np.array([1.0] * D)[None, :]\n",
    "T = 1.0\n",
    "\n",
    "\"Available architectures\"\n",
    "mode = \"FC\"  # FC, Resnet and Naisnet are available\n",
    "activation = \"Sine\"  # Sine, ReLU and Tanh are available\n",
    "model = CallOption(Xi, T, M, N, D, Mm, layers, mode, activation)\n",
    "\n",
    "n_iter = 1 * 10 ** 4\n",
    "lr = 1e-3\n",
    "#%%\n",
    "tot = time.time()\n",
    "print(model.device)\n",
    "graph = model.train(n_iter, lr)\n",
    "print(\"total time:\", time.time() - tot, \"s\")\n",
    "#%%\n",
    "#model.load_model(\"models/CallOption4-256XVAPaper.pth\")\n",
    "#%%\n",
    "n_iter = 5 * 10 ** 3\n",
    "lr = 1e-5\n",
    "#%%\n",
    "tot = time.time()\n",
    "print(model.device)\n",
    "graph = model.train(n_iter, lr)\n",
    "print(\"total time:\", time.time() - tot, \"s\")\n",
    "#%%\n",
    "model.save_model(\"models/CallOption1D-FC-Sine.pth\")\n",
    "\n",
    "np.random.seed(42)\n",
    "t_test, W_test = model.fetch_minibatch()\n",
    "X_pred, Y_pred = model.predict(Xi, t_test, W_test)\n",
    "\n",
    "if type(t_test).__module__ != 'numpy':\n",
    "    t_test = t_test.cpu().numpy()\n",
    "if type(X_pred).__module__ != 'numpy':\n",
    "    X_pred = X_pred.cpu().detach().numpy()\n",
    "if type(Y_pred).__module__ != 'numpy':\n",
    "    Y_pred = Y_pred.cpu().detach().numpy()\n",
    "\n",
    "for i in range(15):\n",
    "    t_test_i, W_test_i = model.fetch_minibatch()\n",
    "    X_pred_i, Y_pred_i = model.predict(Xi, t_test_i, W_test_i)\n",
    "    if type(X_pred_i).__module__ != 'numpy':\n",
    "        X_pred_i = X_pred_i.cpu().detach().numpy()\n",
    "    if type(Y_pred_i).__module__ != 'numpy':\n",
    "        Y_pred_i = Y_pred_i.cpu().detach().numpy()\n",
    "    if type(t_test_i).__module__ != 'numpy':\n",
    "        t_test_i = t_test_i.cpu().numpy()\n",
    "    t_test = np.concatenate((t_test, t_test_i), axis=0)\n",
    "    X_pred = np.concatenate((X_pred, X_pred_i), axis=0)\n",
    "    Y_pred = np.concatenate((Y_pred, Y_pred_i), axis=0)\n",
    "X_pred = X_pred[:500, :]\n",
    "\n",
    "from scipy.stats import multivariate_normal as normal\n",
    "\n",
    "\n",
    "X_preds = X_pred[:, :, 0]\n",
    "\n",
    "\n",
    "\n",
    "def black_scholes_call(S, K, T, r, sigma, q=0):\n",
    "    d1 = (np.log(S / K) + (r - q + 0.5 * sigma ** 2) * T) / (sigma * np.sqrt(T))\n",
    "    d2 = d1 - sigma * np.sqrt(T)\n",
    "    call_price = (S * np.exp(-q * T) * normal.cdf(d1)) - (K * np.exp(-r * T) * normal.cdf(d2))\n",
    "    delta = normal.cdf(d1)\n",
    "    return call_price, delta\n",
    "\n",
    "\n",
    "def calculate_option_prices(X_pred, time_array, K, r, sigma, T, q=0):\n",
    "    rows, cols = X_pred.shape\n",
    "    option_prices = np.zeros((rows, cols))\n",
    "    deltas = np.zeros((rows, cols))\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            S = X_pred[i, j]\n",
    "            t = time_array[j]\n",
    "            time_to_maturity = T - t\n",
    "            if time_to_maturity > 0:\n",
    "                option_prices[i, j], deltas[i, j] = black_scholes_call(S, K, time_to_maturity, r, sigma, q)\n",
    "            else:\n",
    "                option_prices[i, j] = max(S - K, 0)\n",
    "                if S > K:\n",
    "                    deltas[i, j] = 1\n",
    "                elif S == K:\n",
    "                    deltas[i, j] = 0.5\n",
    "                else:\n",
    "                    deltas[i, j] = 0\n",
    "\n",
    "    return option_prices, deltas\n",
    "\n",
    "K = 1.0  # Strike price\n",
    "r = 0.01  # Risk-free interest rate\n",
    "sigma = 0.25  # Volatility\n",
    "q = 0  # Dividend yield (assuming none)\n",
    "T = 1  # Expiry time in years\n",
    "\n",
    "Y_test, Z_test = calculate_option_prices(X_preds, t_test[0], K, r, sigma, T, q)\n",
    "\n",
    "errors = (Y_test[:500] - Y_pred[:500,:,0])**2\n",
    "errors.mean(), errors.std()\n",
    "\n",
    "np.sqrt(errors.mean())\n",
    "\n",
    "graph = model.iteration, model.training_loss\n",
    "#%%\n",
    "def figsize(scale, nplots = 1):\n",
    "    fig_width_pt = 438.17227\n",
    "    inches_per_pt = 1.0/72.27\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale\n",
    "    fig_height = nplots*fig_width*golden_mean\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size\n",
    "#%%\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "plt.plot(graph[0], graph[1])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Value')\n",
    "plt.yscale(\"log\")\n",
    "plt.title('Evolution of the training loss')\n",
    "samples = 5\n",
    "plt.savefig(f\"{D}-dimensional Call Option loss, {model.mode}-{model.activation}.png\")\n",
    "\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "plt.plot(t_test[0:1, :, 0].T, Y_pred[0:1, :, 0].T)\n",
    "plt.plot(t_test[1:samples, :, 0].T, Y_pred[1:samples, :, 0].T)\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$Y_t = u(t,X_t)$')\n",
    "plt.title(str(D) + '-dimensional Call Option, ' + model.mode + \"-\" + model.activation)\n",
    "plt.savefig(f\"{D}-dimensional Call Option pred, {model.mode}-{model.activation}.png\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=figsize(1.0))\n",
    "plt.plot(t_test[0] * 100, Y_pred[0] * 100, 'b', label='Learned $u(t,X_t)$')\n",
    "plt.plot(t_test[0] * 100, Y_test[0] * 100, 'r--', label='Exact $u(t,X_t)$')\n",
    "plt.plot(t_test[0, -1] * 100, Y_test[0, -1] * 100, 'ko', label='$Y_T = u(T,X_T)$')\n",
    "for i in range(7):\n",
    "    plt.plot(t_test[i] * 100, Y_pred[i] * 100, 'b')\n",
    "    plt.plot(t_test[i] * 100, Y_test[i] * 100, 'r--')\n",
    "    plt.plot(t_test[i, -1] * 100, Y_test[i, -1] * 100, 'ko')\n",
    "plt.plot([0], Y_test[0,0] * 100, 'ks', label='$Y_0 = u(0,X_0)$')\n",
    "plt.title(str(D) + '-dimensional Call Option, ' + model.mode + \"-\" + model.activation)\n",
    "plt.legend()\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$Y_t = u(t,X_t)$')\n",
    "plt.savefig(f\"{D}-dimensional Call Option comp, {model.mode}-{model.activation}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BSB100D\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "from BlackScholesBarenblatt import *\n",
    "from FBSNNs import *\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"Algorithms/\"))\n",
    "sys.path.append(os.path.abspath(\"models/\"))\n",
    "\n",
    "M = 100  # number of trajectories (batch size)\n",
    "N = 50  # number of time snapshots\n",
    "D = 100  # number of dimensions\n",
    "Mm = N **(1/5)\n",
    "\n",
    "layers = [D + 1] + 4 * [256] + [1]\n",
    "\n",
    "Xi = np.array([1.0, 0.5] * int(D / 2))[None, :]\n",
    "T = 1.0\n",
    "\n",
    "\"Available architectures\"\n",
    "mode = \"FC\"  # FC and Naisnet are available\n",
    "activation = \"ReLU\"  # Sine, ReLU and Tanh are available\n",
    "model = BlackScholesBarenblatt(Xi, T, M, N, D, Mm, layers, mode, activation)\n",
    "\n",
    "n_iter = 2 * 10 ** 4\n",
    "lr = 1e-3\n",
    "tot = time.time()\n",
    "print(model.device)\n",
    "graph = model.train(n_iter, lr)\n",
    "print(\"total time:\", time.time() - tot, \"s\")\n",
    "#model.load_model(\"models/BlackScholesBarenblatt100DRelu.pth\")\n",
    "n_iter = 5*10**3\n",
    "lr = 1e-5\n",
    "tot = time.time()\n",
    "print(model.device)\n",
    "graph = model.train(n_iter, lr)\n",
    "print(\"total time:\", time.time() - tot, \"s\")\n",
    "\n",
    "\n",
    "def figsize(scale, nplots=1):\n",
    "    fig_width_pt = 438.17227\n",
    "    inches_per_pt = 1.0/72.27\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale\n",
    "    fig_height = nplots*fig_width*golden_mean\n",
    "    fig_size = [fig_width, fig_height]\n",
    "    return fig_size\n",
    "\n",
    "\n",
    "graph = model.iteration, model.training_loss\n",
    "np.random.seed(40)\n",
    "t_test, W_test = model.fetch_minibatch()\n",
    "X_pred, Y_pred = model.predict(Xi, t_test, W_test)\n",
    "samples = 5\n",
    "\n",
    "if type(t_test).__module__ != 'numpy':\n",
    "    t_test = t_test.cpu().numpy()\n",
    "if type(X_pred).__module__ != 'numpy':\n",
    "    X_pred = X_pred.cpu().detach().numpy()\n",
    "if type(Y_pred).__module__ != 'numpy':\n",
    "    Y_pred = Y_pred.cpu().detach().numpy()\n",
    "\n",
    "Y_test = np.reshape(u_exact(T, np.reshape(t_test[0:M, :, :], [-1, 1]), np.reshape(X_pred[0:M, :, :], [-1, D])),\n",
    "                    [M, -1, 1])\n",
    "plt.figure(figsize=figsize(1))\n",
    "plt.plot(graph[0], graph[1])\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Value')\n",
    "plt.yscale(\"log\")\n",
    "plt.title('Evolution of the training loss')\n",
    "plt.savefig(\"Figures/BlackScholesBarenblatt100D_FC_ReLU_Loss.png\")\n",
    "\n",
    "plt.figure(figsize=figsize(1))\n",
    "plt.plot(t_test[0:1, :, 0].T, Y_pred[0:1, :, 0].T,\n",
    "         'b', label='Learned $u(t,X_t)$')\n",
    "plt.plot(t_test[0:1, :, 0].T, Y_test[0:1, :, 0].T,\n",
    "         'r--', label='Exact $u(t,X_t)$')\n",
    "plt.plot(t_test[0:1, -1, 0], Y_test[0:1, -1, 0],\n",
    "         'ko', label='$Y_T = u(T,X_T)$')\n",
    "\n",
    "plt.plot(t_test[1:samples, :, 0].T, Y_pred[1:samples, :, 0].T, 'b')\n",
    "plt.plot(t_test[1:samples, :, 0].T, Y_test[1:samples, :, 0].T, 'r--')\n",
    "plt.plot(t_test[1:samples, -1, 0], Y_test[1:samples, -1, 0], 'ko')\n",
    "\n",
    "plt.plot([0], Y_test[0, 0, 0], 'ks', label='$Y_0 = u(0,X_0)$')\n",
    "\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('$Y_t = u(t,X_t)$')\n",
    "plt.title('D=' + str(D) + ' Black-Scholes-Barenblatt, ' +\n",
    "          model.mode + \"-\" + model.activation)\n",
    "plt.legend()\n",
    "plt.savefig(\"Figures/BlackScholesBarenblatt100D_FC_ReLU_.png\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "errors = np.sqrt((Y_test - Y_pred) ** 2 / Y_test ** 2)\n",
    "mean_errors = np.mean(errors, 0)\n",
    "std_errors = np.std(errors, 0)\n",
    "plt.figure(figsize=figsize(1))\n",
    "plt.plot(t_test[0, :, 0], mean_errors, 'b', label='mean')\n",
    "plt.plot(t_test[0, :, 0], mean_errors + 2 * std_errors,\n",
    "         'r--', label='mean + two standard deviations')\n",
    "plt.xlabel('$t$')\n",
    "plt.ylabel('relative error')\n",
    "plt.title('D=' + str(D) + ' Black-Scholes-Barenblatt, ' +\n",
    "          model.mode + \"-\" + model.activation)\n",
    "plt.legend()\n",
    "plt.savefig(\"Figures/BlackScholesBarenblatt100D_FC_ReLU_Errors.png\")\n",
    "plt.show()\n",
    "#model.save_model(\"models/BlackScholesBarenblatt100D.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
